{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: Amazon Bedrock Knowledge Bases & Advanced RAG\n",
    "\n",
    "**Duration:** 60-75 minutes  \n",
    "**Cost:** < $0.50 (using Claude Haiku)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Build custom knowledge base with chunking strategies\n",
    "2. Implement advanced RAG patterns\n",
    "3. Compare retrieval strategies\n",
    "4. Query decomposition for complex questions\n",
    "5. Multi-turn conversational RAG\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Lab 1\n",
    "- Basic understanding of embeddings and vector search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q boto3 pandas numpy matplotlib seaborn scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"✓ Bedrock client initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Sample Knowledge Base Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expanded AWS documentation for knowledge base\n",
    "sample_documents = {\n",
    "    'compute_services.txt': \"\"\"AWS Compute Services Overview\n",
    "\n",
    "Amazon EC2 provides resizable virtual servers in the cloud. You can launch instances \n",
    "with various configurations of CPU, memory, storage, and networking. EC2 supports \n",
    "multiple operating systems and offers pay-as-you-go pricing. Instance types range \n",
    "from t3.micro for small workloads to c5.24xlarge for compute-intensive applications.\n",
    "\n",
    "AWS Lambda is a serverless compute service that runs code in response to events. \n",
    "Lambda automatically manages compute resources, scaling from a few requests per day \n",
    "to thousands per second. You only pay for compute time consumed. Maximum execution \n",
    "time is 15 minutes per invocation.\n",
    "\n",
    "Amazon ECS is a container orchestration service supporting Docker containers. \n",
    "It allows you to run applications on a managed cluster. ECS eliminates the need \n",
    "to install and operate your own container orchestration software.\"\"\",\n",
    "    \n",
    "    'storage_services.txt': \"\"\"AWS Storage Services Guide\n",
    "\n",
    "Amazon S3 is object storage built to store and retrieve any amount of data. \n",
    "S3 offers 99.999999999% durability and stores data across multiple facilities. \n",
    "Common use cases include backup, archiving, data lakes, and website hosting. \n",
    "Storage classes range from S3 Standard to S3 Glacier Deep Archive.\n",
    "\n",
    "Amazon EBS provides persistent block storage for EC2 instances. EBS volumes \n",
    "are automatically replicated within their Availability Zone. You can create \n",
    "point-in-time snapshots stored in S3. Volume types include gp3, io2, and st1.\n",
    "\n",
    "Amazon EFS is a scalable file storage for EC2 instances. EFS automatically \n",
    "grows and shrinks as you add and remove files. It can be accessed concurrently \n",
    "from multiple EC2 instances across Availability Zones.\"\"\",\n",
    "    \n",
    "    'database_services.txt': \"\"\"AWS Database Services Portfolio\n",
    "\n",
    "Amazon RDS makes it easy to set up and operate relational databases in the cloud. \n",
    "RDS supports MySQL, PostgreSQL, Oracle, SQL Server, and MariaDB. Features include \n",
    "automated backups, software patching, and monitoring. Multi-AZ deployments provide \n",
    "high availability.\n",
    "\n",
    "Amazon DynamoDB is a fast and flexible NoSQL database service. DynamoDB delivers \n",
    "single-digit millisecond performance at any scale. It is fully managed with \n",
    "built-in security, backup, and in-memory caching. Supports both key-value and \n",
    "document data models.\n",
    "\n",
    "Amazon Aurora is a MySQL and PostgreSQL-compatible relational database. Aurora \n",
    "is up to 5x faster than MySQL and 3x faster than PostgreSQL. It provides \n",
    "commercial-grade performance at one-tenth the cost. Features automatic scaling \n",
    "from 10GB to 128TB.\"\"\",\n",
    "    \n",
    "    'ml_services.txt': \"\"\"AWS Machine Learning and AI Services\n",
    "\n",
    "Amazon SageMaker is a fully managed machine learning platform. It provides tools \n",
    "to build, train, and deploy ML models quickly. SageMaker supports popular frameworks \n",
    "like TensorFlow, PyTorch, and scikit-learn. Features include SageMaker Studio, \n",
    "Autopilot, and Model Monitor.\n",
    "\n",
    "Amazon Bedrock provides foundation models from leading AI companies. Access models \n",
    "from Anthropic, AI21 Labs, Stability AI, and Amazon through one API. Bedrock enables \n",
    "building and scaling generative AI applications securely. Supports RAG and fine-tuning.\n",
    "\n",
    "Amazon Rekognition provides image and video analysis using deep learning. It can \n",
    "detect objects, people, text, scenes, and activities. Rekognition also supports \n",
    "facial analysis and face comparison with high accuracy.\"\"\",\n",
    "    \n",
    "    'security_services.txt': \"\"\"AWS Security and Identity Services\n",
    "\n",
    "AWS IAM controls access to AWS resources. IAM lets you create users, groups, and \n",
    "roles with specific permissions. Multi-factor authentication adds extra security. \n",
    "IAM policies are written in JSON format.\n",
    "\n",
    "AWS KMS manages encryption keys for your applications. KMS integrates with most \n",
    "AWS services to encrypt data. You control who can use keys and how they are used. \n",
    "Supports both symmetric and asymmetric keys.\n",
    "\n",
    "AWS Security Hub provides a comprehensive view of security alerts and compliance. \n",
    "It aggregates findings from multiple AWS services and partner solutions. Security \n",
    "Hub automatically checks against security best practices and standards.\"\"\"\n",
    "}\n",
    "\n",
    "print(f\"Created {len(sample_documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Custom Knowledge Base Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvancedKnowledgeBase:\n",
    "    def __init__(self, bedrock_client):\n",
    "        self.client = bedrock_client\n",
    "        self.documents = []\n",
    "        self.embeddings = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def get_embedding(self, text):\n",
    "        \"\"\"Generate embedding using Titan\"\"\"\n",
    "        body = json.dumps({\"inputText\": text})\n",
    "        response = self.client.invoke_model(\n",
    "            modelId='amazon.titan-embed-text-v1',\n",
    "            body=body\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return np.array(response_body['embedding'])\n",
    "    \n",
    "    def chunk_text(self, text, chunk_size=300, overlap=50):\n",
    "        \"\"\"Split text into overlapping chunks\"\"\"\n",
    "        words = text.split()\n",
    "        chunks = []\n",
    "        \n",
    "        for i in range(0, len(words), chunk_size - overlap):\n",
    "            chunk = ' '.join(words[i:i + chunk_size])\n",
    "            if chunk:\n",
    "                chunks.append(chunk)\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def add_document(self, text, metadata=None):\n",
    "        \"\"\"Add document with chunking\"\"\"\n",
    "        chunks = self.chunk_text(text)\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            embedding = self.get_embedding(chunk)\n",
    "            self.documents.append(chunk)\n",
    "            self.embeddings.append(embedding)\n",
    "            \n",
    "            chunk_metadata = metadata.copy() if metadata else {}\n",
    "            chunk_metadata['chunk_id'] = i\n",
    "            chunk_metadata['total_chunks'] = len(chunks)\n",
    "            self.metadata.append(chunk_metadata)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def search(self, query, top_k=5):\n",
    "        \"\"\"Semantic search\"\"\"\n",
    "        query_embedding = self.get_embedding(query).reshape(1, -1)\n",
    "        embeddings_array = np.array(self.embeddings)\n",
    "        \n",
    "        similarities = cosine_similarity(query_embedding, embeddings_array)[0]\n",
    "        top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "        \n",
    "        results = []\n",
    "        for idx in top_indices:\n",
    "            results.append({\n",
    "                'text': self.documents[idx],\n",
    "                'score': float(similarities[idx]),\n",
    "                'metadata': self.metadata[idx]\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def invoke_claude(self, prompt, max_tokens=512):\n",
    "        \"\"\"Invoke Claude Haiku\"\"\"\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        })\n",
    "        \n",
    "        response = self.client.invoke_model(\n",
    "            modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text']\n",
    "\n",
    "kb = AdvancedKnowledgeBase(bedrock_runtime)\n",
    "print(\"✓ Knowledge Base initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load documents\n",
    "print(\"Loading documents into Knowledge Base...\\n\")\n",
    "\n",
    "for filename, content in sample_documents.items():\n",
    "    chunks_added = kb.add_document(\n",
    "        text=content,\n",
    "        metadata={'source': filename, 'type': 'aws_documentation'}\n",
    "    )\n",
    "    print(f\"✓ {filename}: {chunks_added} chunks\")\n",
    "\n",
    "print(f\"\\nTotal chunks: {len(kb.documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic RAG Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(kb, question, top_k=3):\n",
    "    \"\"\"Simple RAG implementation\"\"\"\n",
    "    # Retrieve\n",
    "    search_results = kb.search(question, top_k=top_k)\n",
    "    \n",
    "    # Build context\n",
    "    context_parts = []\n",
    "    for i, result in enumerate(search_results):\n",
    "        source = result['metadata']['source']\n",
    "        context_parts.append(f\"[Source {i+1}: {source}]\\n{result['text']}\")\n",
    "    \n",
    "    context = \"\\n\\n\".join(context_parts)\n",
    "    \n",
    "    # Generate answer\n",
    "    prompt = f\"\"\"Based on the context below, answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer (cite sources using [Source N]):\"\"\"\n",
    "    \n",
    "    answer = kb.invoke_claude(prompt, max_tokens=300)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': search_results\n",
    "    }\n",
    "\n",
    "# Test queries\n",
    "test_queries = [\n",
    "    \"What storage service should I use for a data lake?\",\n",
    "    \"How does Lambda pricing work?\",\n",
    "    \"What is the difference between RDS and DynamoDB?\"\n",
    "]\n",
    "\n",
    "print(\"Basic RAG Results:\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"{'='*80}\")\n",
    "    result = rag_query(kb, query)\n",
    "    print(f\"Q: {result['question']}\\n\")\n",
    "    print(f\"A: {result['answer']}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Query Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decompose_query(kb, complex_query):\n",
    "    \"\"\"Break complex queries into sub-queries\"\"\"\n",
    "    decompose_prompt = f\"\"\"Break this complex question into 2-3 simpler sub-questions.\n",
    "\n",
    "Complex Question: {complex_query}\n",
    "\n",
    "List the sub-questions, one per line:\"\"\"\n",
    "    \n",
    "    sub_questions_text = kb.invoke_claude(decompose_prompt, max_tokens=150)\n",
    "    sub_questions = [q.strip() for q in sub_questions_text.strip().split('\\n') if q.strip() and not q.strip().startswith('#')]\n",
    "    \n",
    "    # Clean up numbered lists\n",
    "    sub_questions = [q.split('. ', 1)[-1] if '. ' in q else q for q in sub_questions]\n",
    "    \n",
    "    return sub_questions[:3]  # Limit to 3\n",
    "\n",
    "# Test\n",
    "complex_query = \"How should I design a scalable web application on AWS with storage and compute?\"\n",
    "sub_questions = decompose_query(kb, complex_query)\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\\n\")\n",
    "print(\"Sub-questions:\")\n",
    "for i, sq in enumerate(sub_questions):\n",
    "    print(f\"  {i+1}. {sq}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Multi-Turn Conversational RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConversationalRAG:\n",
    "    def __init__(self, knowledge_base):\n",
    "        self.kb = knowledge_base\n",
    "        self.history = []\n",
    "    \n",
    "    def query(self, question):\n",
    "        # Get history context\n",
    "        history_context = \"\\n\".join([\n",
    "            f\"Q: {h['question']}\\nA: {h['answer']}\"\n",
    "            for h in self.history[-2:]  # Last 2 turns\n",
    "        ])\n",
    "        \n",
    "        # Search\n",
    "        search_results = self.kb.search(question, top_k=2)\n",
    "        doc_context = \"\\n\\n\".join([r['text'] for r in search_results])\n",
    "        \n",
    "        # Build prompt\n",
    "        prompt = f\"\"\"Answer based on context and conversation history.\n",
    "\n",
    "Previous Conversation:\n",
    "{history_context if history_context else 'None'}\n",
    "\n",
    "Context:\n",
    "{doc_context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "        \n",
    "        answer = self.kb.invoke_claude(prompt, max_tokens=250)\n",
    "        \n",
    "        self.history.append({\n",
    "            'question': question,\n",
    "            'answer': answer\n",
    "        })\n",
    "        \n",
    "        return answer\n",
    "\n",
    "# Test conversation\n",
    "conv_rag = ConversationalRAG(kb)\n",
    "\n",
    "conversation = [\n",
    "    \"What is Amazon S3?\",\n",
    "    \"What are its durability guarantees?\",\n",
    "    \"What are common use cases for it?\"\n",
    "]\n",
    "\n",
    "print(\"Conversational RAG Demo:\\n\")\n",
    "for question in conversation:\n",
    "    print(f\"User: {question}\")\n",
    "    answer = conv_rag.query(question)\n",
    "    print(f\"Assistant: {answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Retrieval Strategy Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_retrieval_strategies(kb, query):\n",
    "    \"\"\"Compare different retrieval approaches\"\"\"\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    # Strategy 1: Top-K only\n",
    "    results_k3 = kb.search(query, top_k=3)\n",
    "    print(\"Top-3 Results:\")\n",
    "    for i, r in enumerate(results_k3):\n",
    "        print(f\"  {i+1}. Score: {r['score']:.4f} - {r['metadata']['source']}\")\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    # Strategy 2: Top-K with threshold\n",
    "    threshold = 0.7\n",
    "    results_threshold = [r for r in kb.search(query, top_k=5) if r['score'] > threshold]\n",
    "    print(f\"Results with score > {threshold}:\")\n",
    "    for i, r in enumerate(results_threshold):\n",
    "        print(f\"  {i+1}. Score: {r['score']:.4f} - {r['metadata']['source']}\")\n",
    "\n",
    "# Test\n",
    "compare_retrieval_strategies(kb, \"What database should I use for high performance?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Benchmarking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_queries = [\n",
    "    \"What is EC2?\",\n",
    "    \"Tell me about Lambda\",\n",
    "    \"Which database for analytics?\",\n",
    "    \"What ML services are available?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "print(\"Benchmarking RAG performance...\\n\")\n",
    "\n",
    "for query in benchmark_queries:\n",
    "    # Time retrieval\n",
    "    start = time.time()\n",
    "    search_results = kb.search(query, top_k=3)\n",
    "    retrieval_time = time.time() - start\n",
    "    \n",
    "    # Time generation\n",
    "    start = time.time()\n",
    "    result = rag_query(kb, query, top_k=3)\n",
    "    total_time = time.time() - start\n",
    "    \n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'retrieval_time': retrieval_time,\n",
    "        'total_time': total_time,\n",
    "        'avg_score': np.mean([r['score'] for r in search_results])\n",
    "    })\n",
    "    print(f\"✓ {query}\")\n",
    "\n",
    "metrics_df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance Metrics:\")\n",
    "print(metrics_df.to_string(index=False))\n",
    "print(f\"\\nAvg retrieval: {metrics_df['retrieval_time'].mean():.3f}s\")\n",
    "print(f\"Avg total: {metrics_df['total_time'].mean():.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot performance metrics\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Response times\n",
    "axes[0].bar(range(len(metrics_df)), metrics_df['total_time'])\n",
    "axes[0].set_xlabel('Query')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('RAG Response Times', fontweight='bold')\n",
    "axes[0].set_xticks(range(len(metrics_df)))\n",
    "axes[0].set_xticklabels([f'Q{i+1}' for i in range(len(metrics_df))])\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Relevance scores\n",
    "axes[1].bar(range(len(metrics_df)), metrics_df['avg_score'])\n",
    "axes[1].set_xlabel('Query')\n",
    "axes[1].set_ylabel('Average Relevance Score')\n",
    "axes[1].set_title('Average Retrieval Scores', fontweight='bold')\n",
    "axes[1].set_xticks(range(len(metrics_df)))\n",
    "axes[1].set_xticklabels([f'Q{i+1}' for i in range(len(metrics_df))])\n",
    "axes[1].set_ylim([0, 1])\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate costs\n",
    "usage = {\n",
    "    'Embeddings': {'calls': 30, 'tokens': 50, 'cost_per_1K': 0.0001},\n",
    "    'Claude Haiku': {'calls': 20, 'input_tokens': 200, 'output_tokens': 250, \n",
    "                     'input_cost': 0.25, 'output_cost': 1.25}\n",
    "}\n",
    "\n",
    "embed_cost = usage['Embeddings']['calls'] * usage['Embeddings']['tokens'] / 1000 * usage['Embeddings']['cost_per_1K']\n",
    "haiku_cost = (\n",
    "    usage['Claude Haiku']['calls'] * usage['Claude Haiku']['input_tokens'] / 1_000_000 * usage['Claude Haiku']['input_cost'] +\n",
    "    usage['Claude Haiku']['calls'] * usage['Claude Haiku']['output_tokens'] / 1_000_000 * usage['Claude Haiku']['output_cost']\n",
    ")\n",
    "\n",
    "total = embed_cost + haiku_cost\n",
    "\n",
    "print(\"Lab 2 Cost Breakdown:\")\n",
    "print(f\"  Embeddings: ${embed_cost:.4f}\")\n",
    "print(f\"  Claude Haiku: ${haiku_cost:.4f}\")\n",
    "print(f\"  Total: ${total:.4f}\")\n",
    "print(\"\\n✓ Well under budget!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "- ✅ Building custom knowledge bases with chunking\n",
    "- ✅ Implementing advanced RAG patterns\n",
    "- ✅ Query decomposition for complex questions\n",
    "- ✅ Conversational RAG with memory\n",
    "- ✅ Comparing retrieval strategies\n",
    "- ✅ Performance benchmarking\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Chunking strategy impacts retrieval quality\n",
    "2. Query decomposition helps with complex questions\n",
    "3. Conversation history improves multi-turn interactions\n",
    "4. Multiple retrieval strategies have different trade-offs\n",
    "\n",
    "**Next Steps:**\n",
    "- Lab 3: LLM Evaluation & Agentic AI"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
