{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1: Amazon Bedrock Fundamentals & Embeddings\n",
    "\n",
    "**Duration:** 45-60 minutes  \n",
    "**Cost:** < $0.50 (using Claude Haiku & Titan Embeddings)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Set up Amazon Bedrock client and invoke foundation models\n",
    "2. Work with Claude Haiku for cost-effective text generation\n",
    "3. Generate and use embeddings with Titan Embeddings\n",
    "4. Implement semantic search using vector similarity\n",
    "5. Compare different embedding approaches\n",
    "\n",
    "## Prerequisites\n",
    "- AWS Account with Bedrock access\n",
    "- Model access enabled for: Claude Haiku, Titan Embeddings\n",
    "- IAM permissions for Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q boto3 numpy scikit-learn pandas matplotlib seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # Change to your preferred region\n",
    ")\n",
    "\n",
    "bedrock = boto3.client(\n",
    "    service_name='bedrock',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"✓ Bedrock clients initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Claude Haiku (Cost-Effective LLM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_claude_haiku(prompt, max_tokens=512, temperature=0.7):\n",
    "    \"\"\"\n",
    "    Invoke Claude Haiku - most cost-effective Claude model\n",
    "    Pricing: ~$0.25 per 1M input tokens, ~$1.25 per 1M output tokens\n",
    "    \"\"\"\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"temperature\": temperature,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return response_body['content'][0]['text']\n",
    "\n",
    "# Test Claude Haiku\n",
    "test_prompt = \"Explain Amazon Bedrock in 2 sentences.\"\n",
    "response = invoke_claude_haiku(test_prompt, max_tokens=100)\n",
    "print(f\"Prompt: {test_prompt}\\n\")\n",
    "print(f\"Response: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Titan Embeddings - Generating Vector Representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(text, model_id='amazon.titan-embed-text-v1'):\n",
    "    \"\"\"\n",
    "    Generate embeddings using Titan Embeddings\n",
    "    Pricing: ~$0.0001 per 1K tokens (very cost-effective)\n",
    "    Returns 1536-dimensional vector\n",
    "    \"\"\"\n",
    "    body = json.dumps({\n",
    "        \"inputText\": text\n",
    "    })\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId=model_id,\n",
    "        body=body\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    return np.array(response_body['embedding'])\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"Amazon Bedrock is a fully managed service for foundation models.\"\n",
    "embedding = get_embedding(test_text)\n",
    "print(f\"Text: {test_text}\")\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 10 values: {embedding[:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Semantic Search Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample knowledge base - AWS services descriptions\n",
    "documents = [\n",
    "    \"Amazon S3 is an object storage service offering scalability, data availability, security, and performance.\",\n",
    "    \"Amazon EC2 provides secure, resizable compute capacity in the cloud as virtual servers.\",\n",
    "    \"Amazon RDS makes it easy to set up, operate, and scale a relational database in the cloud.\",\n",
    "    \"AWS Lambda lets you run code without provisioning or managing servers, paying only for compute time.\",\n",
    "    \"Amazon DynamoDB is a fast and flexible NoSQL database service for any scale.\",\n",
    "    \"Amazon Bedrock offers foundation models from leading AI companies through a single API.\",\n",
    "    \"Amazon SageMaker helps build, train, and deploy machine learning models quickly.\",\n",
    "    \"Amazon CloudWatch monitors AWS resources and applications in real-time.\",\n",
    "    \"Amazon VPC lets you provision a logically isolated section of the AWS Cloud.\",\n",
    "    \"Amazon SNS is a fully managed messaging service for both application-to-application and application-to-person communication.\"\n",
    "]\n",
    "\n",
    "print(f\"Knowledge base created with {len(documents)} documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate embeddings for all documents\n",
    "print(\"Generating embeddings for knowledge base...\")\n",
    "document_embeddings = []\n",
    "\n",
    "for i, doc in enumerate(documents):\n",
    "    embedding = get_embedding(doc)\n",
    "    document_embeddings.append(embedding)\n",
    "    print(f\"✓ Document {i+1}/{len(documents)} embedded\")\n",
    "\n",
    "document_embeddings = np.array(document_embeddings)\n",
    "print(f\"\\nEmbeddings matrix shape: {document_embeddings.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query, top_k=3):\n",
    "    \"\"\"\n",
    "    Perform semantic search using cosine similarity\n",
    "    \"\"\"\n",
    "    # Get query embedding\n",
    "    query_embedding = get_embedding(query).reshape(1, -1)\n",
    "    \n",
    "    # Calculate cosine similarity\n",
    "    similarities = cosine_similarity(query_embedding, document_embeddings)[0]\n",
    "    \n",
    "    # Get top K results\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_indices:\n",
    "        results.append({\n",
    "            'document': documents[idx],\n",
    "            'similarity': similarities[idx],\n",
    "            'rank': len(results) + 1\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test semantic search\n",
    "queries = [\n",
    "    \"How can I store files in the cloud?\",\n",
    "    \"I need a database that scales automatically\",\n",
    "    \"What service helps with machine learning?\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Query: {query}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    results = semantic_search(query, top_k=3)\n",
    "    \n",
    "    for result in results:\n",
    "        print(f\"\\n[Rank {result['rank']}] Similarity: {result['similarity']:.4f}\")\n",
    "        print(f\"Document: {result['document']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. RAG (Retrieval-Augmented Generation) Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_query(question, top_k=2):\n",
    "    \"\"\"\n",
    "    Implement simple RAG pattern:\n",
    "    1. Retrieve relevant documents using semantic search\n",
    "    2. Augment prompt with retrieved context\n",
    "    3. Generate answer using Claude Haiku\n",
    "    \"\"\"\n",
    "    # Retrieve relevant documents\n",
    "    search_results = semantic_search(question, top_k=top_k)\n",
    "    \n",
    "    # Build context from retrieved documents\n",
    "    context = \"\\n\\n\".join([r['document'] for r in search_results])\n",
    "    \n",
    "    # Create augmented prompt\n",
    "    prompt = f\"\"\"Based on the following context, please answer the question.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer concisely and only use information from the context provided.\"\"\"\n",
    "    \n",
    "    # Generate answer\n",
    "    answer = invoke_claude_haiku(prompt, max_tokens=200)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'context': search_results,\n",
    "        'answer': answer\n",
    "    }\n",
    "\n",
    "# Test RAG\n",
    "questions = [\n",
    "    \"What AWS service should I use for object storage?\",\n",
    "    \"Which service is best for serverless computing?\",\n",
    "    \"How can I monitor my AWS resources?\"\n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    result = rag_query(question)\n",
    "    \n",
    "    print(f\"Question: {result['question']}\\n\")\n",
    "    print(f\"Retrieved Context:\")\n",
    "    for ctx in result['context']:\n",
    "        print(f\"  - {ctx['document']} (similarity: {ctx['similarity']:.4f})\")\n",
    "    \n",
    "    print(f\"\\nAnswer: {result['answer']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Embedding Analysis & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate similarity matrix\n",
    "similarity_matrix = cosine_similarity(document_embeddings)\n",
    "\n",
    "# Create heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(similarity_matrix, \n",
    "            annot=True, \n",
    "            fmt='.2f', \n",
    "            cmap='coolwarm',\n",
    "            xticklabels=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
    "            yticklabels=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
    "            cbar_kws={'label': 'Cosine Similarity'})\n",
    "\n",
    "plt.title('Document Similarity Matrix', fontsize=16, pad=20)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print document labels\n",
    "print(\"\\nDocument Index:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Doc {i+1}: {doc[:60]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization of embeddings\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce to 2D\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(document_embeddings)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], s=200, alpha=0.6, c=range(len(documents)), cmap='tab10')\n",
    "\n",
    "# Add labels\n",
    "for i, doc in enumerate(documents):\n",
    "    plt.annotate(f\"Doc {i+1}\", \n",
    "                (embeddings_2d[i, 0], embeddings_2d[i, 1]),\n",
    "                fontsize=10,\n",
    "                xytext=(5, 5),\n",
    "                textcoords='offset points')\n",
    "\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)', fontsize=12)\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)', fontsize=12)\n",
    "plt.title('2D Visualization of Document Embeddings (PCA)', fontsize=14, pad=15)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Total variance explained: {sum(pca.explained_variance_ratio_):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Comparing Different Embedding Models (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_embedding_models(text):\n",
    "    \"\"\"\n",
    "    Compare Titan Embed V1 and V2 (if available)\n",
    "    \"\"\"\n",
    "    models = {\n",
    "        'Titan V1': 'amazon.titan-embed-text-v1',\n",
    "        'Titan V2': 'amazon.titan-embed-text-v2:0'\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for name, model_id in models.items():\n",
    "        try:\n",
    "            embedding = get_embedding(text, model_id=model_id)\n",
    "            results[name] = {\n",
    "                'dimension': len(embedding),\n",
    "                'embedding': embedding,\n",
    "                'model_id': model_id\n",
    "            }\n",
    "            print(f\"✓ {name}: {len(embedding)} dimensions\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ {name}: Not available - {str(e)}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "test_text = \"Machine learning and artificial intelligence\"\n",
    "comparison = compare_embedding_models(test_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cost Tracking & Optimization Tips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimated costs for this lab\n",
    "estimated_costs = {\n",
    "    'Claude Haiku Invocations': {\n",
    "        'count': 10,\n",
    "        'avg_input_tokens': 100,\n",
    "        'avg_output_tokens': 150,\n",
    "        'input_cost_per_1M': 0.25,\n",
    "        'output_cost_per_1M': 1.25\n",
    "    },\n",
    "    'Titan Embeddings': {\n",
    "        'count': 20,\n",
    "        'avg_tokens': 50,\n",
    "        'cost_per_1K': 0.0001\n",
    "    }\n",
    "}\n",
    "\n",
    "# Calculate costs\n",
    "haiku = estimated_costs['Claude Haiku Invocations']\n",
    "haiku_cost = (\n",
    "    (haiku['count'] * haiku['avg_input_tokens'] / 1_000_000 * haiku['input_cost_per_1M']) +\n",
    "    (haiku['count'] * haiku['avg_output_tokens'] / 1_000_000 * haiku['output_cost_per_1M'])\n",
    ")\n",
    "\n",
    "embeddings = estimated_costs['Titan Embeddings']\n",
    "embeddings_cost = (\n",
    "    embeddings['count'] * embeddings['avg_tokens'] / 1_000 * embeddings['cost_per_1K']\n",
    ")\n",
    "\n",
    "total_cost = haiku_cost + embeddings_cost\n",
    "\n",
    "print(\"Estimated Lab Costs:\")\n",
    "print(f\"  Claude Haiku: ${haiku_cost:.4f}\")\n",
    "print(f\"  Titan Embeddings: ${embeddings_cost:.4f}\")\n",
    "print(f\"  Total: ${total_cost:.4f}\")\n",
    "print(\"\\nCost Optimization Tips:\")\n",
    "print(\"  1. Use Claude Haiku instead of Sonnet/Opus for simple tasks\")\n",
    "print(\"  2. Cache embeddings instead of regenerating them\")\n",
    "print(\"  3. Batch API calls when possible\")\n",
    "print(\"  4. Use appropriate max_tokens limits\")\n",
    "print(\"  5. Monitor usage with CloudWatch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Exercise: Build Your Own Semantic Search\n",
    "\n",
    "**Task:** Create a semantic search system for your own domain\n",
    "\n",
    "1. Create a list of 10-15 documents about a topic of your choice\n",
    "2. Generate embeddings for all documents\n",
    "3. Implement semantic search function\n",
    "4. Test with 3-5 queries\n",
    "5. Visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here\n",
    "my_documents = [\n",
    "    # Add your documents\n",
    "]\n",
    "\n",
    "# Generate embeddings\n",
    "\n",
    "# Implement search\n",
    "\n",
    "# Test queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "- ✅ How to invoke Amazon Bedrock foundation models\n",
    "- ✅ Using Claude Haiku for cost-effective text generation\n",
    "- ✅ Generating embeddings with Titan Embeddings\n",
    "- ✅ Implementing semantic search with cosine similarity\n",
    "- ✅ Building a simple RAG system\n",
    "- ✅ Visualizing embeddings and similarities\n",
    "- ✅ Understanding Bedrock pricing and optimization\n",
    "\n",
    "**Next Steps:**\n",
    "- Lab 2: Amazon Bedrock Knowledge Bases & Advanced RAG\n",
    "- Lab 3: LLM Evaluation & Agentic AI\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Amazon Bedrock Documentation](https://docs.aws.amazon.com/bedrock/)\n",
    "- [Bedrock Pricing](https://aws.amazon.com/bedrock/pricing/)\n",
    "- [Bedrock Workshop](https://github.com/aws-samples/amazon-bedrock-workshop)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
