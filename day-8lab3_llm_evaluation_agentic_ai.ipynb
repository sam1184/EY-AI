{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: LLM Evaluation & Agentic AI with Bedrock\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Cost:** < $1.00 (using Claude Haiku + evaluation metrics)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Evaluate LLM outputs using multiple metrics\n",
    "2. Compare different foundation models\n",
    "3. Implement prompt engineering best practices\n",
    "4. Build agentic AI workflows with tool use\n",
    "5. Create self-improving AI agents\n",
    "\n",
    "## Prerequisites\n",
    "- Completion of Labs 1 & 2\n",
    "- Understanding of LLM concepts\n",
    "- Basic Python and AWS knowledge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LLM Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q boto3 pandas numpy matplotlib seaborn scikit-learn nltk textstat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import textstat\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('punkt', quiet=True)\n",
    "\n",
    "# Initialize Bedrock\n",
    "bedrock_runtime = boto3.client('bedrock-runtime', region_name='us-east-1')\n",
    "bedrock = boto3.client('bedrock', region_name='us-east-1')\n",
    "\n",
    "print(\"✓ Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. LLM Invocation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockModelEvaluator:\n",
    "    \"\"\"\n",
    "    Utility class for invoking and evaluating Bedrock models\n",
    "    \"\"\"\n",
    "    def __init__(self, bedrock_runtime_client):\n",
    "        self.client = bedrock_runtime_client\n",
    "        self.models = {\n",
    "            'claude-haiku': 'anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            'claude-sonnet': 'anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "            'titan-express': 'amazon.titan-text-express-v1',\n",
    "        }\n",
    "    \n",
    "    def invoke_claude(self, prompt, model_key='claude-haiku', max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Invoke Claude models\"\"\"\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"temperature\": temperature,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = self.client.invoke_model(\n",
    "            modelId=self.models[model_key],\n",
    "            body=body\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        text = response_body['content'][0]['text']\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'latency': latency,\n",
    "            'model': model_key,\n",
    "            'input_tokens': response_body.get('usage', {}).get('input_tokens', 0),\n",
    "            'output_tokens': response_body.get('usage', {}).get('output_tokens', 0)\n",
    "        }\n",
    "    \n",
    "    def invoke_titan(self, prompt, max_tokens=512, temperature=0.7):\n",
    "        \"\"\"Invoke Titan models\"\"\"\n",
    "        body = json.dumps({\n",
    "            \"inputText\": prompt,\n",
    "            \"textGenerationConfig\": {\n",
    "                \"maxTokenCount\": max_tokens,\n",
    "                \"temperature\": temperature,\n",
    "                \"topP\": 0.9\n",
    "            }\n",
    "        })\n",
    "        \n",
    "        start_time = time.time()\n",
    "        response = self.client.invoke_model(\n",
    "            modelId=self.models['titan-express'],\n",
    "            body=body\n",
    "        )\n",
    "        latency = time.time() - start_time\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        text = response_body['results'][0]['outputText']\n",
    "        \n",
    "        return {\n",
    "            'text': text,\n",
    "            'latency': latency,\n",
    "            'model': 'titan-express',\n",
    "            'input_tokens': response_body.get('inputTextTokenCount', 0),\n",
    "            'output_tokens': response_body['results'][0].get('tokenCount', 0)\n",
    "        }\n",
    "    \n",
    "    def invoke_model(self, prompt, model_key='claude-haiku', **kwargs):\n",
    "        \"\"\"Universal model invocation\"\"\"\n",
    "        if 'claude' in model_key:\n",
    "            return self.invoke_claude(prompt, model_key, **kwargs)\n",
    "        elif 'titan' in model_key:\n",
    "            return self.invoke_titan(prompt, **kwargs)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_key}\")\n",
    "\n",
    "evaluator = BedrockModelEvaluator(bedrock_runtime)\n",
    "print(\"✓ Model evaluator initialized\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMMetrics:\n",
    "    \"\"\"\n",
    "    Comprehensive LLM evaluation metrics\n",
    "    \"\"\"\n",
    "    @staticmethod\n",
    "    def readability_score(text):\n",
    "        \"\"\"Flesch Reading Ease score (0-100, higher = easier)\"\"\"\n",
    "        try:\n",
    "            return textstat.flesch_reading_ease(text)\n",
    "        except:\n",
    "            return 0\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_embedding(text, bedrock_client):\n",
    "        \"\"\"Get Titan embedding\"\"\"\n",
    "        body = json.dumps({\"inputText\": text})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId='amazon.titan-embed-text-v1',\n",
    "            body=body\n",
    "        )\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return np.array(response_body['embedding'])\n",
    "    \n",
    "    @staticmethod\n",
    "    def semantic_similarity(text1, text2, bedrock_client):\n",
    "        \"\"\"Cosine similarity between embeddings\"\"\"\n",
    "        emb1 = LLMMetrics.get_embedding(text1, bedrock_client).reshape(1, -1)\n",
    "        emb2 = LLMMetrics.get_embedding(text2, bedrock_client).reshape(1, -1)\n",
    "        return cosine_similarity(emb1, emb2)[0][0]\n",
    "    \n",
    "    @staticmethod\n",
    "    def coherence_score(text):\n",
    "        \"\"\"Simple coherence based on sentence count and avg length\"\"\"\n",
    "        sentences = nltk.sent_tokenize(text)\n",
    "        if not sentences:\n",
    "            return 0\n",
    "        avg_length = np.mean([len(s.split()) for s in sentences])\n",
    "        # Ideal sentence length 15-20 words\n",
    "        return 1 / (1 + abs(avg_length - 17.5) / 10)\n",
    "    \n",
    "    @staticmethod\n",
    "    def factuality_check(text, bedrock_client, reference_facts):\n",
    "        \"\"\"\n",
    "        LLM-as-judge factuality check\n",
    "        \"\"\"\n",
    "        prompt = f\"\"\"Evaluate the factual accuracy of the following text.\n",
    "\n",
    "Reference Facts:\n",
    "{reference_facts}\n",
    "\n",
    "Text to Evaluate:\n",
    "{text}\n",
    "\n",
    "Rate factual accuracy on a scale of 0-10 where:\n",
    "0 = Completely inaccurate\n",
    "10 = Perfectly accurate\n",
    "\n",
    "Respond with only a number between 0 and 10.\"\"\"\n",
    "        \n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": 10,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        })\n",
    "        \n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        score_text = response_body['content'][0]['text'].strip()\n",
    "        \n",
    "        try:\n",
    "            return float(score_text) / 10  # Normalize to 0-1\n",
    "        except:\n",
    "            return 0.5\n",
    "    \n",
    "    @staticmethod\n",
    "    def evaluate_response(response, ground_truth=None, reference_facts=None, bedrock_client=None):\n",
    "        \"\"\"Comprehensive evaluation\"\"\"\n",
    "        text = response['text']\n",
    "        \n",
    "        metrics = {\n",
    "            'model': response['model'],\n",
    "            'latency': response['latency'],\n",
    "            'output_tokens': response['output_tokens'],\n",
    "            'readability': LLMMetrics.readability_score(text),\n",
    "            'coherence': LLMMetrics.coherence_score(text),\n",
    "            'length': len(text.split())\n",
    "        }\n",
    "        \n",
    "        if ground_truth and bedrock_client:\n",
    "            metrics['semantic_similarity'] = LLMMetrics.semantic_similarity(\n",
    "                text, ground_truth, bedrock_client\n",
    "            )\n",
    "        \n",
    "        if reference_facts and bedrock_client:\n",
    "            metrics['factuality'] = LLMMetrics.factuality_check(\n",
    "                text, bedrock_client, reference_facts\n",
    "            )\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "print(\"✓ Metrics class created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Model Comparison Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test prompt and ground truth\n",
    "test_prompt = \"Explain what Amazon S3 is and its main use cases in 2-3 sentences.\"\n",
    "\n",
    "ground_truth = \"\"\"Amazon S3 (Simple Storage Service) is a scalable object storage service \n",
    "that stores data as objects within buckets. It's commonly used for backup and restore, \n",
    "data archiving, content distribution, data lakes, and hosting static websites.\"\"\"\n",
    "\n",
    "reference_facts = \"\"\"Amazon S3 is object storage. It stores data in buckets. \n",
    "Common uses include backup, archiving, data lakes, and static website hosting.\"\"\"\n",
    "\n",
    "# Test models\n",
    "models_to_test = ['claude-haiku', 'titan-express']\n",
    "\n",
    "print(\"Running model comparison...\\n\")\n",
    "results = []\n",
    "\n",
    "for model in models_to_test:\n",
    "    print(f\"Testing {model}...\")\n",
    "    \n",
    "    response = evaluator.invoke_model(\n",
    "        test_prompt,\n",
    "        model_key=model,\n",
    "        max_tokens=200,\n",
    "        temperature=0.7\n",
    "    )\n",
    "    \n",
    "    metrics = LLMMetrics.evaluate_response(\n",
    "        response,\n",
    "        ground_truth=ground_truth,\n",
    "        reference_facts=reference_facts,\n",
    "        bedrock_client=bedrock_runtime\n",
    "    )\n",
    "    \n",
    "    metrics['response_text'] = response['text']\n",
    "    results.append(metrics)\n",
    "    \n",
    "    print(f\"✓ {model} complete\\n\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"Model Comparison Results:\")\n",
    "print(comparison_df[['model', 'latency', 'readability', 'coherence', \n",
    "                     'semantic_similarity', 'factuality', 'output_tokens']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Latency comparison\n",
    "axes[0, 0].bar(comparison_df['model'], comparison_df['latency'])\n",
    "axes[0, 0].set_title('Response Latency', fontsize=12, fontweight='bold')\n",
    "axes[0, 0].set_ylabel('Seconds')\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Quality metrics\n",
    "quality_metrics = ['readability', 'coherence', 'semantic_similarity', 'factuality']\n",
    "x = np.arange(len(models_to_test))\n",
    "width = 0.2\n",
    "\n",
    "for i, metric in enumerate(quality_metrics):\n",
    "    if metric in comparison_df.columns:\n",
    "        normalized = comparison_df[metric] / comparison_df[metric].max()\n",
    "        axes[0, 1].bar(x + i*width, normalized, width, label=metric)\n",
    "\n",
    "axes[0, 1].set_title('Quality Metrics (Normalized)', fontsize=12, fontweight='bold')\n",
    "axes[0, 1].set_xticks(x + width * 1.5)\n",
    "axes[0, 1].set_xticklabels(comparison_df['model'])\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Token usage\n",
    "axes[1, 0].bar(comparison_df['model'], comparison_df['output_tokens'])\n",
    "axes[1, 0].set_title('Output Tokens', fontsize=12, fontweight='bold')\n",
    "axes[1, 0].set_ylabel('Token Count')\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Overall score (composite)\n",
    "comparison_df['overall_score'] = (\n",
    "    comparison_df['readability'] / 100 * 0.2 +\n",
    "    comparison_df['coherence'] * 0.2 +\n",
    "    comparison_df.get('semantic_similarity', 0) * 0.3 +\n",
    "    comparison_df.get('factuality', 0) * 0.3\n",
    ")\n",
    "\n",
    "axes[1, 1].bar(comparison_df['model'], comparison_df['overall_score'])\n",
    "axes[1, 1].set_title('Overall Score (Composite)', fontsize=12, fontweight='bold')\n",
    "axes[1, 1].set_ylabel('Score (0-1)')\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGenerated Responses:\")\n",
    "for i, row in comparison_df.iterrows():\n",
    "    print(f\"\\n{row['model']}:\")\n",
    "    print(row['response_text'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Prompt Engineering Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test different prompt strategies\n",
    "base_task = \"Explain Amazon Lambda\"\n",
    "\n",
    "prompt_strategies = {\n",
    "    'Basic': base_task,\n",
    "    \n",
    "    'Structured': f\"\"\"{base_task}.\n",
    "\n",
    "Provide your answer in the following format:\n",
    "1. Definition\n",
    "2. Key features\n",
    "3. Use cases\"\"\",\n",
    "    \n",
    "    'Few-shot': f\"\"\"Here are examples of good service explanations:\n",
    "\n",
    "Example 1: Amazon S3 is object storage that provides scalability and durability. \n",
    "It's used for backup, archiving, and data lakes.\n",
    "\n",
    "Example 2: Amazon EC2 provides virtual servers in the cloud. \n",
    "It offers flexible compute capacity for various workloads.\n",
    "\n",
    "Now, {base_task} following the same style.\"\"\",\n",
    "    \n",
    "    'Chain-of-Thought': f\"\"\"{base_task}.\n",
    "\n",
    "Think step by step:\n",
    "1. First, what type of service is it?\n",
    "2. Then, what problems does it solve?\n",
    "3. Finally, what are typical use cases?\n",
    "\n",
    "Provide a concise answer covering these points.\"\"\"\n",
    "}\n",
    "\n",
    "print(\"Evaluating prompt strategies...\\n\")\n",
    "prompt_results = []\n",
    "\n",
    "for strategy_name, prompt in prompt_strategies.items():\n",
    "    response = evaluator.invoke_model(prompt, max_tokens=250)\n",
    "    metrics = LLMMetrics.evaluate_response(response, bedrock_client=bedrock_runtime)\n",
    "    metrics['strategy'] = strategy_name\n",
    "    metrics['response_text'] = response['text']\n",
    "    prompt_results.append(metrics)\n",
    "    print(f\"✓ {strategy_name} strategy evaluated\")\n",
    "\n",
    "prompt_df = pd.DataFrame(prompt_results)\n",
    "print(\"\\nPrompt Strategy Comparison:\")\n",
    "print(prompt_df[['strategy', 'readability', 'coherence', 'length']].to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Agentic AI with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Tool Use / Function Calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for the agent\n",
    "tools = [\n",
    "    {\n",
    "        \"name\": \"get_weather\",\n",
    "        \"description\": \"Get current weather for a location. Use this when the user asks about weather.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"City name or location\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"calculate\",\n",
    "        \"description\": \"Perform mathematical calculations. Use this for math problems.\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"expression\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Mathematical expression to evaluate\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"expression\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"search_documentation\",\n",
    "        \"description\": \"Search AWS documentation for information about services\",\n",
    "        \"input_schema\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"service\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"AWS service name\"\n",
    "                },\n",
    "                \"topic\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Specific topic or question\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"service\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"✓ Tools defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tool implementations (mock)\n",
    "def get_weather(location):\n",
    "    \"\"\"Mock weather API\"\"\"\n",
    "    return {\n",
    "        \"location\": location,\n",
    "        \"temperature\": 72,\n",
    "        \"condition\": \"Partly cloudy\",\n",
    "        \"humidity\": 65\n",
    "    }\n",
    "\n",
    "def calculate(expression):\n",
    "    \"\"\"Safe calculator\"\"\"\n",
    "    try:\n",
    "        # Basic safety check\n",
    "        if any(char in expression for char in ['__', 'import', 'exec', 'eval']):\n",
    "            return {\"error\": \"Invalid expression\"}\n",
    "        result = eval(expression, {\"__builtins__\": {}}, {})\n",
    "        return {\"expression\": expression, \"result\": result}\n",
    "    except Exception as e:\n",
    "        return {\"error\": str(e)}\n",
    "\n",
    "def search_documentation(service, topic=None):\n",
    "    \"\"\"Mock documentation search\"\"\"\n",
    "    docs = {\n",
    "        \"S3\": \"Amazon S3 provides object storage with high durability and availability.\",\n",
    "        \"Lambda\": \"AWS Lambda runs code without provisioning servers. Pay only for compute time.\",\n",
    "        \"EC2\": \"Amazon EC2 provides resizable virtual servers in the cloud.\"\n",
    "    }\n",
    "    return {\n",
    "        \"service\": service,\n",
    "        \"topic\": topic,\n",
    "        \"documentation\": docs.get(service, \"Service documentation not found.\")\n",
    "    }\n",
    "\n",
    "# Tool registry\n",
    "tool_functions = {\n",
    "    \"get_weather\": get_weather,\n",
    "    \"calculate\": calculate,\n",
    "    \"search_documentation\": search_documentation\n",
    "}\n",
    "\n",
    "print(\"✓ Tool functions implemented\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Agentic AI Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BedrockAgent:\n",
    "    \"\"\"\n",
    "    Simple agentic AI using Claude with tool use\n",
    "    \"\"\"\n",
    "    def __init__(self, bedrock_client, tools, tool_functions, max_iterations=5):\n",
    "        self.client = bedrock_client\n",
    "        self.tools = tools\n",
    "        self.tool_functions = tool_functions\n",
    "        self.max_iterations = max_iterations\n",
    "        self.conversation_history = []\n",
    "    \n",
    "    def run(self, user_message):\n",
    "        \"\"\"\n",
    "        Run agentic loop with tool use\n",
    "        \"\"\"\n",
    "        self.conversation_history.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": user_message\n",
    "        })\n",
    "        \n",
    "        iterations = 0\n",
    "        execution_log = []\n",
    "        \n",
    "        while iterations < self.max_iterations:\n",
    "            iterations += 1\n",
    "            execution_log.append(f\"\\n--- Iteration {iterations} ---\")\n",
    "            \n",
    "            # Call Claude with tools\n",
    "            body = json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1024,\n",
    "                \"tools\": self.tools,\n",
    "                \"messages\": self.conversation_history\n",
    "            })\n",
    "            \n",
    "            response = self.client.invoke_model(\n",
    "                modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "                body=body\n",
    "            )\n",
    "            \n",
    "            response_body = json.loads(response['body'].read())\n",
    "            \n",
    "            # Check stop reason\n",
    "            stop_reason = response_body.get('stop_reason')\n",
    "            \n",
    "            if stop_reason == 'end_turn':\n",
    "                # Claude finished without tools\n",
    "                final_text = response_body['content'][0]['text']\n",
    "                execution_log.append(f\"Final response: {final_text}\")\n",
    "                return {\n",
    "                    'response': final_text,\n",
    "                    'iterations': iterations,\n",
    "                    'log': execution_log\n",
    "                }\n",
    "            \n",
    "            elif stop_reason == 'tool_use':\n",
    "                # Claude wants to use tools\n",
    "                assistant_content = response_body['content']\n",
    "                self.conversation_history.append({\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": assistant_content\n",
    "                })\n",
    "                \n",
    "                # Execute tools\n",
    "                tool_results = []\n",
    "                \n",
    "                for content_block in assistant_content:\n",
    "                    if content_block.get('type') == 'tool_use':\n",
    "                        tool_name = content_block['name']\n",
    "                        tool_input = content_block['input']\n",
    "                        tool_use_id = content_block['id']\n",
    "                        \n",
    "                        execution_log.append(f\"Calling tool: {tool_name}\")\n",
    "                        execution_log.append(f\"Input: {json.dumps(tool_input)}\")\n",
    "                        \n",
    "                        # Execute tool\n",
    "                        if tool_name in self.tool_functions:\n",
    "                            result = self.tool_functions[tool_name](**tool_input)\n",
    "                            execution_log.append(f\"Result: {json.dumps(result)}\")\n",
    "                            \n",
    "                            tool_results.append({\n",
    "                                \"type\": \"tool_result\",\n",
    "                                \"tool_use_id\": tool_use_id,\n",
    "                                \"content\": json.dumps(result)\n",
    "                            })\n",
    "                \n",
    "                # Add tool results to conversation\n",
    "                self.conversation_history.append({\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": tool_results\n",
    "                })\n",
    "            \n",
    "            else:\n",
    "                # Unexpected stop reason\n",
    "                execution_log.append(f\"Unexpected stop reason: {stop_reason}\")\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'response': 'Max iterations reached',\n",
    "            'iterations': iterations,\n",
    "            'log': execution_log\n",
    "        }\n",
    "\n",
    "# Initialize agent\n",
    "agent = BedrockAgent(bedrock_runtime, tools, tool_functions)\n",
    "print(\"✓ Bedrock Agent initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "test_queries = [\n",
    "    \"What's the weather in Seattle?\",\n",
    "    \"Calculate 125 * 48 + 1000\",\n",
    "    \"Tell me about AWS Lambda pricing\"\n",
    "]\n",
    "\n",
    "print(\"Testing Agentic AI...\\n\")\n",
    "\n",
    "for query in test_queries:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"User: {query}\\n\")\n",
    "    \n",
    "    # Reset agent for each query\n",
    "    agent = BedrockAgent(bedrock_runtime, tools, tool_functions)\n",
    "    result = agent.run(query)\n",
    "    \n",
    "    print(f\"Agent Response: {result['response']}\")\n",
    "    print(f\"\\nExecution Log:\")\n",
    "    for log_entry in result['log']:\n",
    "        print(log_entry)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Multi-Step Reasoning Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex task requiring multiple steps\n",
    "complex_task = \"\"\"I need to deploy a web application on AWS. \n",
    "Can you help me understand the costs? \n",
    "I expect 10,000 requests per day, need to store 100GB of data, \n",
    "and want automatic scaling.\"\"\"\n",
    "\n",
    "print(\"Complex Multi-Step Task:\\n\")\n",
    "print(f\"Task: {complex_task}\\n\")\n",
    "\n",
    "# Add AWS cost calculator tool\n",
    "def estimate_aws_costs(service, usage):\n",
    "    \"\"\"Mock cost estimator\"\"\"\n",
    "    pricing = {\n",
    "        'Lambda': {'per_million_requests': 0.20, 'per_gb_second': 0.0000166667},\n",
    "        'S3': {'per_gb_month': 0.023},\n",
    "        'ALB': {'per_hour': 0.0225, 'per_lcu_hour': 0.008}\n",
    "    }\n",
    "    \n",
    "    if service not in pricing:\n",
    "        return {\"error\": f\"Pricing for {service} not available\"}\n",
    "    \n",
    "    return {\n",
    "        \"service\": service,\n",
    "        \"usage\": usage,\n",
    "        \"estimated_cost\": \"Varies based on specific usage patterns\",\n",
    "        \"pricing_info\": pricing[service]\n",
    "    }\n",
    "\n",
    "# Add cost estimation tool\n",
    "extended_tools = tools + [{\n",
    "    \"name\": \"estimate_aws_costs\",\n",
    "    \"description\": \"Estimate AWS service costs based on usage\",\n",
    "    \"input_schema\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"service\": {\"type\": \"string\", \"description\": \"AWS service name\"},\n",
    "            \"usage\": {\"type\": \"string\", \"description\": \"Usage description\"}\n",
    "        },\n",
    "        \"required\": [\"service\", \"usage\"]\n",
    "    }\n",
    "}]\n",
    "\n",
    "extended_functions = tool_functions.copy()\n",
    "extended_functions['estimate_aws_costs'] = estimate_aws_costs\n",
    "\n",
    "# Run complex task\n",
    "complex_agent = BedrockAgent(bedrock_runtime, extended_tools, extended_functions, max_iterations=10)\n",
    "result = complex_agent.run(complex_task)\n",
    "\n",
    "print(f\"Agent Response:\\n{result['response']}\\n\")\n",
    "print(f\"Total Iterations: {result['iterations']}\\n\")\n",
    "print(\"Execution Log:\")\n",
    "for log_entry in result['log']:\n",
    "    print(log_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Self-Improving Agent with Reflection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReflectiveAgent:\n",
    "    \"\"\"\n",
    "    Agent that reflects on and improves its responses\n",
    "    \"\"\"\n",
    "    def __init__(self, bedrock_client):\n",
    "        self.client = bedrock_client\n",
    "    \n",
    "    def generate_response(self, prompt, max_tokens=512):\n",
    "        \"\"\"Generate initial response\"\"\"\n",
    "        body = json.dumps({\n",
    "            \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "            \"max_tokens\": max_tokens,\n",
    "            \"messages\": [{\"role\": \"user\", \"content\": prompt}]\n",
    "        })\n",
    "        \n",
    "        response = self.client.invoke_model(\n",
    "            modelId='anthropic.claude-3-haiku-20240307-v1:0',\n",
    "            body=body\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['content'][0]['text']\n",
    "    \n",
    "    def reflect(self, original_prompt, response):\n",
    "        \"\"\"Reflect on the response and identify improvements\"\"\"\n",
    "        reflection_prompt = f\"\"\"Review this response and identify areas for improvement.\n",
    "\n",
    "Original Question: {original_prompt}\n",
    "\n",
    "Response: {response}\n",
    "\n",
    "Provide 2-3 specific improvements that could make this response better.\n",
    "Consider: clarity, completeness, accuracy, and helpfulness.\"\"\"\n",
    "        \n",
    "        return self.generate_response(reflection_prompt, max_tokens=300)\n",
    "    \n",
    "    def improve(self, original_prompt, response, reflection):\n",
    "        \"\"\"Generate improved response based on reflection\"\"\"\n",
    "        improvement_prompt = f\"\"\"Improve the following response based on this reflection.\n",
    "\n",
    "Original Question: {original_prompt}\n",
    "\n",
    "Initial Response: {response}\n",
    "\n",
    "Reflection: {reflection}\n",
    "\n",
    "Provide an improved response that addresses the identified issues.\"\"\"\n",
    "        \n",
    "        return self.generate_response(improvement_prompt, max_tokens=512)\n",
    "    \n",
    "    def run_with_reflection(self, prompt, iterations=2):\n",
    "        \"\"\"Run agent with self-reflection and improvement\"\"\"\n",
    "        print(f\"Question: {prompt}\\n\")\n",
    "        \n",
    "        # Initial response\n",
    "        response = self.generate_response(prompt)\n",
    "        print(f\"Initial Response:\\n{response}\\n\")\n",
    "        print(f\"{'-'*80}\\n\")\n",
    "        \n",
    "        # Reflection and improvement loop\n",
    "        for i in range(iterations):\n",
    "            print(f\"Reflection Iteration {i+1}:\\n\")\n",
    "            \n",
    "            reflection = self.reflect(prompt, response)\n",
    "            print(f\"Reflection:\\n{reflection}\\n\")\n",
    "            \n",
    "            response = self.improve(prompt, response, reflection)\n",
    "            print(f\"Improved Response:\\n{response}\\n\")\n",
    "            print(f\"{'-'*80}\\n\")\n",
    "        \n",
    "        return response\n",
    "\n",
    "# Test reflective agent\n",
    "reflective_agent = ReflectiveAgent(bedrock_runtime)\n",
    "\n",
    "test_question = \"Explain the difference between Amazon RDS and DynamoDB.\"\n",
    "\n",
    "print(\"Self-Improving Agent Demo:\\n\")\n",
    "print(\"=\"*80)\n",
    "final_response = reflective_agent.run_with_reflection(test_question, iterations=1)\n",
    "\n",
    "print(f\"Final Optimized Response:\\n{final_response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate lab costs\n",
    "estimated_usage = {\n",
    "    'Model Comparison (Claude Haiku)': {\n",
    "        'calls': 10,\n",
    "        'avg_input_tokens': 100,\n",
    "        'avg_output_tokens': 200,\n",
    "        'input_cost_per_1M': 0.25,\n",
    "        'output_cost_per_1M': 1.25\n",
    "    },\n",
    "    'Titan Express': {\n",
    "        'calls': 5,\n",
    "        'avg_input_tokens': 100,\n",
    "        'avg_output_tokens': 200,\n",
    "        'input_cost_per_1M': 0.20,\n",
    "        'output_cost_per_1M': 0.60\n",
    "    },\n",
    "    'Embeddings (Titan)': {\n",
    "        'calls': 30,\n",
    "        'avg_tokens': 50,\n",
    "        'cost_per_1K': 0.0001\n",
    "    },\n",
    "    'Agentic AI (Claude Haiku)': {\n",
    "        'calls': 20,\n",
    "        'avg_input_tokens': 200,\n",
    "        'avg_output_tokens': 300,\n",
    "        'input_cost_per_1M': 0.25,\n",
    "        'output_cost_per_1M': 1.25\n",
    "    }\n",
    "}\n",
    "\n",
    "total_cost = 0\n",
    "\n",
    "print(\"Lab 3 Cost Breakdown:\\n\")\n",
    "print(f\"{'Component':<30} {'Calls':<10} {'Cost':<10}\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "for component, usage in estimated_usage.items():\n",
    "    if 'Embeddings' in component:\n",
    "        cost = usage['calls'] * usage['avg_tokens'] / 1000 * usage['cost_per_1K']\n",
    "    else:\n",
    "        cost = (\n",
    "            (usage['calls'] * usage['avg_input_tokens'] / 1_000_000 * usage['input_cost_per_1M']) +\n",
    "            (usage['calls'] * usage['avg_output_tokens'] / 1_000_000 * usage['output_cost_per_1M'])\n",
    "        )\n",
    "    \n",
    "    total_cost += cost\n",
    "    print(f\"{component:<30} {usage['calls']:<10} ${cost:.4f}\")\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(f\"{'Total Estimated Cost':<40} ${total_cost:.4f}\")\n",
    "\n",
    "print(\"\\n✓ Well under $1.00 budget!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "**LLM Evaluation:**\n",
    "- ✅ Implementing multiple evaluation metrics (readability, coherence, factuality)\n",
    "- ✅ Comparing different foundation models\n",
    "- ✅ Testing prompt engineering strategies\n",
    "- ✅ Using LLM-as-judge for quality assessment\n",
    "\n",
    "**Agentic AI:**\n",
    "- ✅ Implementing tool use / function calling\n",
    "- ✅ Building multi-step reasoning agents\n",
    "- ✅ Creating self-improving agents with reflection\n",
    "- ✅ Orchestrating complex workflows\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. Evaluation is critical for production LLM systems\n",
    "2. Different models excel at different tasks\n",
    "3. Prompt engineering significantly impacts quality\n",
    "4. Agentic patterns enable complex problem-solving\n",
    "5. Reflection loops can improve output quality\n",
    "\n",
    "**Next Steps:**\n",
    "- Implement custom evaluation metrics for your domain\n",
    "- Build production-ready agents with error handling\n",
    "- Explore Bedrock Agents for managed agentic workflows\n",
    "- Integrate with your applications\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Bedrock Model Evaluation](https://docs.aws.amazon.com/bedrock/latest/userguide/model-evaluation.html)\n",
    "- [Bedrock Agents](https://docs.aws.amazon.com/bedrock/latest/userguide/agents.html)\n",
    "- [Claude Tool Use Guide](https://docs.anthropic.com/claude/docs/tool-use)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
