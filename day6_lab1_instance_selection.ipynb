{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Day 6 Lab 1: Instance Selection & Cost Optimization\n",
    "\n",
    "## üéØ Learning Objectives\n",
    "- Understand T-series vs M-series instances\n",
    "- Compare costs and performance\n",
    "- Monitor CloudWatch metrics\n",
    "- Make informed instance selection decisions\n",
    "\n",
    "## üè¶ Banking Use Case\n",
    "Deploy a **credit risk scoring model** on different instance types to find optimal cost/performance balance.\n",
    "\n",
    "## ‚è±Ô∏è Duration: 30 minutes\n",
    "## üí∞ Cost: ~$0.07"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.sklearn import SKLearnModel\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import json\n",
    "\n",
    "# Initialize\n",
    "session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "bucket = session.default_bucket()\n",
    "\n",
    "print(f\"Region: {region}\")\n",
    "print(f\"Role: {role}\")\n",
    "print(f\"Bucket: {bucket}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Understanding Instance Types\n",
    "\n",
    "**Key Learning:** T-series instances can ONLY be used for endpoints, NOT training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instance type comparison\n",
    "instance_comparison = {\n",
    "    'Instance Type': ['ml.t3.medium', 'ml.t3.large', 'ml.m5.large', 'ml.m5.xlarge', 'ml.c5.xlarge'],\n",
    "    'vCPU': [2, 2, 2, 4, 4],\n",
    "    'Memory (GB)': [4, 8, 8, 16, 8],\n",
    "    'Cost/Hour': ['$0.05', '$0.10', '$0.115', '$0.23', '$0.204'],\n",
    "    'Training': ['‚ùå', '‚ùå', '‚úÖ', '‚úÖ', '‚úÖ'],\n",
    "    'Endpoint': ['‚úÖ', '‚úÖ', '‚úÖ', '‚úÖ', '‚úÖ'],\n",
    "    'Best For': ['Dev/Test', 'Low Traffic', 'Production', 'High Traffic', 'CPU Intensive']\n",
    "}\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(instance_comparison)\n",
    "print(\"\\nüìä SageMaker Instance Type Comparison:\\n\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Key Insights:\")\n",
    "print(\"  1. T-series: ONLY for endpoints (NOT training)\")\n",
    "print(\"  2. M-series: For BOTH training and endpoints\")\n",
    "print(\"  3. C-series: CPU-optimized for inference\")\n",
    "print(\"  4. P-series: GPU instances for deep learning (not shown)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Cost Analysis Scenarios\n",
    "\n",
    "**Key Learning:** Calculate costs for different traffic patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cost calculation for different scenarios\n",
    "def calculate_monthly_cost(instance_type, cost_per_hour, hours_per_day=24):\n",
    "    daily_cost = cost_per_hour * hours_per_day\n",
    "    monthly_cost = daily_cost * 30\n",
    "    return monthly_cost\n",
    "\n",
    "scenarios = {\n",
    "    'Scenario': ['Dev/Test (8hrs/day)', 'Low Traffic (24/7)', 'Production (24/7)', 'High Traffic (24/7)'],\n",
    "    'Instance': ['ml.t3.medium', 'ml.t3.large', 'ml.m5.large', 'ml.m5.xlarge'],\n",
    "    'Hours/Day': [8, 24, 24, 24],\n",
    "    'Cost/Hour': [0.05, 0.10, 0.115, 0.23],\n",
    "    'Monthly Cost': [\n",
    "        f\"${calculate_monthly_cost('t3.medium', 0.05, 8):.2f}\",\n",
    "        f\"${calculate_monthly_cost('t3.large', 0.10, 24):.2f}\",\n",
    "        f\"${calculate_monthly_cost('m5.large', 0.115, 24):.2f}\",\n",
    "        f\"${calculate_monthly_cost('m5.xlarge', 0.23, 24):.2f}\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_scenarios = pd.DataFrame(scenarios)\n",
    "print(\"\\nüí∞ Monthly Cost Scenarios:\\n\")\n",
    "print(df_scenarios.to_string(index=False))\n",
    "\n",
    "print(\"\\nüìä Cost Optimization Tips:\")\n",
    "print(\"  1. Use T3 for dev/test: Save 50-60% vs M5\")\n",
    "print(\"  2. Auto-scaling: Scale down during off-hours\")\n",
    "print(\"  3. Spot instances: 70% savings for training (not endpoints)\")\n",
    "print(\"  4. Right-sizing: Monitor CloudWatch, adjust as needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Performance vs Cost Trade-offs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated performance comparison (based on AWS benchmarks)\n",
    "performance_data = {\n",
    "    'Instance': ['ml.t3.medium', 'ml.t3.large', 'ml.m5.large', 'ml.m5.xlarge', 'ml.c5.xlarge'],\n",
    "    'Avg Latency (ms)': [45, 40, 35, 30, 25],\n",
    "    'P95 Latency (ms)': [80, 70, 60, 50, 40],\n",
    "    'Max TPS': [50, 100, 150, 300, 400],\n",
    "    'Cost/Hour': [0.05, 0.10, 0.115, 0.23, 0.204],\n",
    "    'Cost per 1M requests': ['$2.50', '$2.78', '$2.11', '$2.11', '$1.41']\n",
    "}\n",
    "\n",
    "df_perf = pd.DataFrame(performance_data)\n",
    "print(\"\\n‚ö° Performance vs Cost Analysis:\\n\")\n",
    "print(df_perf.to_string(index=False))\n",
    "\n",
    "print(\"\\nüéØ Decision Framework:\")\n",
    "print(\"\\n  Use T3 when:\")\n",
    "print(\"    - Dev/test environments\")\n",
    "print(\"    - < 100 requests/second\")\n",
    "print(\"    - Latency < 100ms acceptable\")\n",
    "print(\"    - Cost is primary concern\")\n",
    "print(\"\\n  Use M5 when:\")\n",
    "print(\"    - Production workloads\")\n",
    "print(\"    - 100-300 requests/second\")\n",
    "print(\"    - Latency < 50ms required\")\n",
    "print(\"    - Consistent performance needed\")\n",
    "print(\"\\n  Use C5 when:\")\n",
    "print(\"    - CPU-intensive models\")\n",
    "print(\"    - > 300 requests/second\")\n",
    "print(\"    - Latency < 30ms required\")\n",
    "print(\"    - Best cost per request\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: CloudWatch Metrics for Right-sizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key CloudWatch metrics to monitor\n",
    "metrics_guide = {\n",
    "    'Metric': [\n",
    "        'CPUUtilization',\n",
    "        'MemoryUtilization',\n",
    "        'ModelLatency',\n",
    "        'Invocations',\n",
    "        'Invocation4XXErrors',\n",
    "        'Invocation5XXErrors'\n",
    "    ],\n",
    "    'Target Range': [\n",
    "        '50-70%',\n",
    "        '< 85%',\n",
    "        '< 100ms',\n",
    "        'Monitor trend',\n",
    "        '< 1%',\n",
    "        '< 0.1%'\n",
    "    ],\n",
    "    'Action if Outside Range': [\n",
    "        'Scale up/down instance',\n",
    "        'Increase instance size',\n",
    "        'Optimize model or scale up',\n",
    "        'Add auto-scaling',\n",
    "        'Check input validation',\n",
    "        'Check model health'\n",
    "    ]\n",
    "}\n",
    "\n",
    "df_metrics = pd.DataFrame(metrics_guide)\n",
    "print(\"\\nüìà CloudWatch Metrics Guide:\\n\")\n",
    "print(df_metrics.to_string(index=False))\n",
    "\n",
    "print(\"\\nüí° Right-sizing Process:\")\n",
    "print(\"  1. Deploy on smallest instance (T3)\")\n",
    "print(\"  2. Monitor for 24-48 hours\")\n",
    "print(\"  3. Check CPU/Memory utilization\")\n",
    "print(\"  4. If CPU > 70%: Scale up to M5\")\n",
    "print(\"  5. If CPU < 30%: Scale down or use auto-scaling\")\n",
    "print(\"  6. Monitor latency and error rates\")\n",
    "print(\"  7. Adjust based on business requirements\")\n",
    "\n",
    "print(\"\\nüîî Set CloudWatch Alarms for:\")\n",
    "print(\"  - CPU > 80% for 5 minutes\")\n",
    "print(\"  - Memory > 85% for 5 minutes\")\n",
    "print(\"  - Latency > 100ms for 5 minutes\")\n",
    "print(\"  - Error rate > 1% for 5 minutes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Real-world Banking Scenario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SecureBank credit risk scoring scenario\n",
    "print(\"üè¶ SecureBank Credit Risk Scoring System\\n\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Scenario parameters\n",
    "daily_loan_applications = 500\n",
    "peak_hours = 8  # 9am-5pm\n",
    "peak_multiplier = 3  # 3x traffic during peak\n",
    "\n",
    "# Calculate requirements\n",
    "avg_requests_per_hour = daily_loan_applications / 24\n",
    "peak_requests_per_hour = avg_requests_per_hour * peak_multiplier\n",
    "peak_requests_per_second = peak_requests_per_hour / 3600\n",
    "\n",
    "print(f\"\\nüìä Traffic Pattern:\")\n",
    "print(f\"  Daily applications: {daily_loan_applications}\")\n",
    "print(f\"  Average: {avg_requests_per_hour:.1f} requests/hour\")\n",
    "print(f\"  Peak (9am-5pm): {peak_requests_per_hour:.1f} requests/hour\")\n",
    "print(f\"  Peak: {peak_requests_per_second:.2f} requests/second\")\n",
    "\n",
    "# Instance recommendations\n",
    "print(f\"\\nüí° Instance Recommendation:\")\n",
    "if peak_requests_per_second < 1:\n",
    "    recommended = \"ml.t3.medium\"\n",
    "    cost = 0.05\n",
    "    reason = \"Low traffic, T3 sufficient\"\n",
    "elif peak_requests_per_second < 3:\n",
    "    recommended = \"ml.m5.large\"\n",
    "    cost = 0.115\n",
    "    reason = \"Moderate traffic, M5 for consistency\"\n",
    "else:\n",
    "    recommended = \"ml.m5.xlarge\"\n",
    "    cost = 0.23\n",
    "    reason = \"High traffic, need more capacity\"\n",
    "\n",
    "monthly_cost = cost * 24 * 30\n",
    "\n",
    "print(f\"  Recommended: {recommended}\")\n",
    "print(f\"  Reason: {reason}\")\n",
    "print(f\"  Monthly cost: ${monthly_cost:.2f}\")\n",
    "\n",
    "# Auto-scaling option\n",
    "print(f\"\\nüîÑ Auto-scaling Alternative:\")\n",
    "off_peak_cost = 0.05 * 16 * 30  # T3 for 16 hours\n",
    "peak_cost = 0.115 * 8 * 30  # M5 for 8 hours\n",
    "autoscaling_cost = off_peak_cost + peak_cost\n",
    "\n",
    "print(f\"  Off-peak (16hrs): ml.t3.medium @ ${off_peak_cost:.2f}/month\")\n",
    "print(f\"  Peak (8hrs): ml.m5.large @ ${peak_cost:.2f}/month\")\n",
    "print(f\"  Total: ${autoscaling_cost:.2f}/month\")\n",
    "print(f\"  Savings: ${monthly_cost - autoscaling_cost:.2f}/month ({((monthly_cost - autoscaling_cost)/monthly_cost*100):.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: Instance Selection Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n‚úÖ Instance Selection Checklist:\\n\")\n",
    "print(\"1. Determine workload type:\")\n",
    "print(\"   ‚ñ° Training ‚Üí Use M5/C5/P3 (NOT T3)\")\n",
    "print(\"   ‚ñ° Endpoint ‚Üí Can use T3/M5/C5\")\n",
    "print(\"\\n2. Estimate traffic:\")\n",
    "print(\"   ‚ñ° < 50 req/sec ‚Üí T3\")\n",
    "print(\"   ‚ñ° 50-200 req/sec ‚Üí M5\")\n",
    "print(\"   ‚ñ° > 200 req/sec ‚Üí C5 or multiple instances\")\n",
    "print(\"\\n3. Check latency requirements:\")\n",
    "print(\"   ‚ñ° < 30ms ‚Üí C5 or GPU\")\n",
    "print(\"   ‚ñ° < 50ms ‚Üí M5\")\n",
    "print(\"   ‚ñ° < 100ms ‚Üí T3 acceptable\")\n",
    "print(\"\\n4. Consider cost:\")\n",
    "print(\"   ‚ñ° Dev/test ‚Üí Start with T3\")\n",
    "print(\"   ‚ñ° Production ‚Üí M5 for consistency\")\n",
    "print(\"   ‚ñ° High traffic ‚Üí C5 for best cost/performance\")\n",
    "print(\"\\n5. Plan for scaling:\")\n",
    "print(\"   ‚ñ° Variable traffic ‚Üí Enable auto-scaling\")\n",
    "print(\"   ‚ñ° Predictable peaks ‚Üí Schedule scaling\")\n",
    "print(\"   ‚ñ° Steady traffic ‚Üí Fixed instance count\")\n",
    "print(\"\\n6. Monitor and optimize:\")\n",
    "print(\"   ‚ñ° Set CloudWatch alarms\")\n",
    "print(\"   ‚ñ° Review metrics weekly\")\n",
    "print(\"   ‚ñ° Right-size based on data\")\n",
    "print(\"   ‚ñ° Test before scaling down\")\n",
    "\n",
    "print(\"\\nüí° Remember: Start small, monitor, and scale up as needed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Key Takeaways\n",
    "\n",
    "1. **T-series instances:**\n",
    "   - ‚úÖ ONLY for endpoints (NOT training)\n",
    "   - ‚úÖ Lowest cost ($0.05/hour)\n",
    "   - ‚úÖ Good for dev/test and low-traffic\n",
    "   - ‚ö†Ô∏è Burstable performance\n",
    "\n",
    "2. **M-series instances:**\n",
    "   - ‚úÖ For BOTH training and endpoints\n",
    "   - ‚úÖ Consistent performance\n",
    "   - ‚úÖ Good for production\n",
    "   - ‚ö†Ô∏è 2-3x more expensive\n",
    "\n",
    "3. **Right-sizing:**\n",
    "   - Monitor CloudWatch metrics\n",
    "   - Target 50-70% CPU utilization\n",
    "   - Balance cost vs performance\n",
    "   - Use auto-scaling for variable traffic\n",
    "\n",
    "4. **Cost optimization:**\n",
    "   - Start with smallest instance\n",
    "   - Scale up only if needed\n",
    "   - Use Spot instances for training (70% savings)\n",
    "   - Delete unused endpoints immediately"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
