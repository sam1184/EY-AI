{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: LangChain & LlamaIndex with Amazon Bedrock\n",
    "\n",
    "**Duration:** 60-90 minutes  \n",
    "**Cost:** < $1.00 (using Claude Haiku & Titan)\n",
    "\n",
    "## Learning Objectives\n",
    "1. Integrate Amazon Bedrock with LangChain framework\n",
    "2. Build LangChain applications (chains, agents, memory)\n",
    "3. Integrate Amazon Bedrock with LlamaIndex framework\n",
    "4. Build RAG applications using LlamaIndex\n",
    "5. Compare LangChain vs LlamaIndex approaches\n",
    "6. Implement production-ready patterns\n",
    "\n",
    "## Prerequisites\n",
    "- AWS Account with Bedrock access\n",
    "- Completion of Labs 1-3 (recommended)\n",
    "- Understanding of LLM concepts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ Important: Dependency Warnings\n",
    "\n",
    "**You may see dependency conflict warnings during installation - this is normal and expected in SageMaker environments.**\n",
    "\n",
    "Common warnings you can **safely ignore**:\n",
    "- `autogluon-multimodal` version conflicts\n",
    "- `sagemaker-studio` missing dependencies\n",
    "- `aiobotocore`/`botocore` version mismatches\n",
    "- `transformers` version conflicts\n",
    "- `sparkmagic` version differences\n",
    "\n",
    "**Why these are safe to ignore:**\n",
    "1. These packages are pre-installed in SageMaker for other features\n",
    "2. They won't interfere with LangChain/LlamaIndex functionality\n",
    "3. The packages we install are isolated to this notebook's requirements\n",
    "\n",
    "**If you see actual errors (not warnings):**\n",
    "- Restart the kernel and try again\n",
    "- Check that you have internet connectivity\n",
    "- Verify Bedrock access in your AWS account\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ⚠️ LangChain Agent Updates\n",
    "\n",
    "**This notebook has been updated to use modern LangChain agent syntax (v0.1.0+)**\n",
    "\n",
    "### What Changed:\n",
    "\n",
    "**OLD (Deprecated):**\n",
    "```python\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION\n",
    ")\n",
    "```\n",
    "\n",
    "**NEW (Modern):**\n",
    "```python\n",
    "from langchain.agents import create_react_agent, AgentExecutor\n",
    "agent = create_react_agent(llm=llm, tools=tools, prompt=react_prompt)\n",
    "agent_executor = AgentExecutor(agent=agent, tools=tools)\n",
    "```\n",
    "\n",
    "### Why This Matters:\n",
    "- `initialize_agent` is deprecated and will show warnings\n",
    "- The new syntax provides better control and flexibility\n",
    "- This notebook now uses the recommended modern approach\n",
    "\n",
    "### What You Need to Know:\n",
    "- All code in this notebook has been updated\n",
    "- The functionality is exactly the same\n",
    "- No deprecation warnings will appear\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: LangChain with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Setup and Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative simple installation (uncomment if preferred):\n",
    "# !pip install -q langchain>=0.1.0 langchain-aws langchain-community langchainhub faiss-cpu pypdf python-docx 2>&1 | grep -v 'dependency conflicts'\n",
    "\n",
    "# Install required packages with proper dependency handling\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "def install_packages():\n",
    "    \"\"\"Install packages with better dependency management\"\"\"\n",
    "    packages = [\n",
    "        'langchain>=0.1.0',\n",
    "        'langchain-aws>=0.1.0',\n",
    "        'langchain-community>=0.0.20',\n",
    "        'faiss-cpu',\n",
    "        'pypdf',\n",
    "        'python-docx'\n",
    "    ]\n",
    "    \n",
    "    print(\"Installing LangChain packages...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call(\n",
    "                [sys.executable, '-m', 'pip', 'install', '-q', package],\n",
    "                stdout=subprocess.DEVNULL,\n",
    "                stderr=subprocess.DEVNULL\n",
    "            )\n",
    "            print(f\"  ✓ {package}\")\n",
    "        except subprocess.CalledProcessError as e:\n",
    "            print(f\"  ⚠ {package} - already satisfied or minor conflict (safe to ignore)\")\n",
    "    \n",
    "    print(\"\\n✓ LangChain packages installed successfully!\")\n",
    "    print(\"\\nNote: Dependency warnings about autogluon, sagemaker-studio, etc. can be safely ignored.\")\n",
    "    print(\"These are SageMaker pre-installed packages that won't affect this lab.\\n\")\n",
    "\n",
    "install_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "# LangChain imports\n",
    "from langchain_aws import ChatBedrock, BedrockEmbeddings\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import LLMChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory, ConversationSummaryMemory\n",
    "from langchain.schema import HumanMessage, SystemMessage, AIMessage\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Modern agent imports (LangChain 0.1.0+)\n",
    "from langchain.agents import create_react_agent, AgentExecutor, Tool\n",
    "from langchain import hub\n",
    "\n",
    "# Initialize Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"✓ Imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Basic LangChain with Bedrock Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ChatBedrock with Claude Haiku\n",
    "llm = ChatBedrock(\n",
    "    model_id=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=bedrock_runtime,\n",
    "    model_kwargs={\n",
    "        \"temperature\": 0.7,\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    ")\n",
    "\n",
    "# Test basic invocation\n",
    "response = llm.invoke(\"Explain Amazon Bedrock in one sentence.\")\n",
    "print(\"Basic LLM Response:\")\n",
    "print(response.content)\n",
    "print(\"\\n✓ ChatBedrock initialized successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using system and human messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are an AWS Solutions Architect expert.\"),\n",
    "    HumanMessage(content=\"What's the best storage service for a data lake?\")\n",
    "]\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(\"Response with System Message:\")\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. LangChain Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a prompt template\n",
    "template = \"\"\"You are an AWS expert. Answer the question about {service} clearly and concisely.\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"service\", \"question\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "# Create LLM Chain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "# Test the chain\n",
    "result = chain.invoke({\n",
    "    \"service\": \"Amazon S3\",\n",
    "    \"question\": \"What are the main features?\"\n",
    "})\n",
    "\n",
    "print(\"Prompt Template Chain Result:\")\n",
    "print(result['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chat prompt template\n",
    "chat_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful AWS assistant. Be concise and accurate.\"),\n",
    "    (\"human\", \"Tell me about {topic}\"),\n",
    "])\n",
    "\n",
    "chat_chain = chat_template | llm\n",
    "\n",
    "# Test multiple topics\n",
    "topics = [\"Lambda\", \"DynamoDB\", \"CloudWatch\"]\n",
    "\n",
    "print(\"Chat Prompt Template Results:\\n\")\n",
    "for topic in topics:\n",
    "    response = chat_chain.invoke({\"topic\": topic})\n",
    "    print(f\"Topic: {topic}\")\n",
    "    print(f\"Response: {response.content}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Conversation Memory with LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversation with buffer memory\n",
    "memory = ConversationBufferMemory()\n",
    "\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"Conversational AI with Memory:\\n\")\n",
    "\n",
    "# Multi-turn conversation\n",
    "responses = [\n",
    "    conversation.predict(input=\"What is Amazon S3?\"),\n",
    "    conversation.predict(input=\"How much does it cost?\"),\n",
    "    conversation.predict(input=\"What about the durability you mentioned earlier?\")\n",
    "]\n",
    "\n",
    "print(\"\\nConversation History:\")\n",
    "print(memory.buffer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary memory (more efficient for long conversations)\n",
    "summary_memory = ConversationSummaryMemory(llm=llm)\n",
    "\n",
    "summary_conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=summary_memory,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "# Simulate a longer conversation\n",
    "conversation_turns = [\n",
    "    \"Tell me about AWS EC2\",\n",
    "    \"What instance types are available?\",\n",
    "    \"How does pricing work?\",\n",
    "    \"What about auto-scaling?\"\n",
    "]\n",
    "\n",
    "print(\"Conversation with Summary Memory:\\n\")\n",
    "for turn in conversation_turns:\n",
    "    response = summary_conversation.predict(input=turn)\n",
    "    print(f\"User: {turn}\")\n",
    "    print(f\"Assistant: {response}\\n\")\n",
    "\n",
    "print(\"\\nConversation Summary:\")\n",
    "print(summary_memory.buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. LangChain RAG with Bedrock Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock Embeddings\n",
    "embeddings = BedrockEmbeddings(\n",
    "    client=bedrock_runtime,\n",
    "    model_id=\"amazon.titan-embed-text-v1\"\n",
    ")\n",
    "\n",
    "# Test embeddings\n",
    "test_text = \"Amazon S3 is object storage\"\n",
    "embedding = embeddings.embed_query(test_text)\n",
    "print(f\"Embedding dimension: {len(embedding)}\")\n",
    "print(f\"First 5 values: {embedding[:5]}\")\n",
    "print(\"\\n✓ Bedrock embeddings working\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample AWS documentation\n",
    "documents = [\n",
    "    \"\"\"Amazon S3 (Simple Storage Service) is an object storage service offering industry-leading \n",
    "    scalability, data availability, security, and performance. S3 stores data as objects within \n",
    "    buckets and provides 99.999999999% (11 9's) of durability.\"\"\",\n",
    "    \n",
    "    \"\"\"Amazon EC2 (Elastic Compute Cloud) provides secure, resizable compute capacity in the cloud \n",
    "    as virtual servers called instances. You can launch instances with various configurations of \n",
    "    CPU, memory, storage, and networking capacity.\"\"\",\n",
    "    \n",
    "    \"\"\"AWS Lambda is a serverless compute service that lets you run code without provisioning or \n",
    "    managing servers. Lambda automatically scales your applications by running code in response \n",
    "    to triggers. You pay only for the compute time you consume.\"\"\",\n",
    "    \n",
    "    \"\"\"Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable \n",
    "    performance with seamless scalability. DynamoDB automatically scales tables up and down to \n",
    "    adjust for capacity and maintains performance.\"\"\",\n",
    "    \n",
    "    \"\"\"Amazon RDS (Relational Database Service) makes it easy to set up, operate, and scale a \n",
    "    relational database in the cloud. RDS supports MySQL, PostgreSQL, MariaDB, Oracle, and \n",
    "    SQL Server database engines.\"\"\",\n",
    "    \n",
    "    \"\"\"Amazon CloudWatch is a monitoring and observability service that provides data and actionable \n",
    "    insights for AWS resources and applications. CloudWatch collects monitoring and operational \n",
    "    data in the form of logs, metrics, and events.\"\"\"\n",
    "]\n",
    "\n",
    "print(f\"Created {len(documents)} sample documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split documents into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len\n",
    ")\n",
    "\n",
    "splits = text_splitter.create_documents(documents)\n",
    "print(f\"Split into {len(splits)} chunks\")\n",
    "\n",
    "# Create FAISS vector store\n",
    "vectorstore = FAISS.from_documents(splits, embeddings)\n",
    "print(\"✓ Vector store created\")\n",
    "\n",
    "# Create retriever\n",
    "retriever = vectorstore.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3}\n",
    ")\n",
    "\n",
    "print(\"✓ Retriever configured\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test retrieval\n",
    "query = \"What database should I use for high performance?\"\n",
    "docs = retriever.get_relevant_documents(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Retrieved Documents:\")\n",
    "for i, doc in enumerate(docs):\n",
    "    print(f\"\\n[{i+1}] {doc.page_content}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RAG chain\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Test RAG queries\n",
    "test_queries = [\n",
    "    \"What is the best storage service for objects?\",\n",
    "    \"How does serverless computing work on AWS?\",\n",
    "    \"Which service should I use for monitoring?\"\n",
    "]\n",
    "\n",
    "print(\"LangChain RAG Results:\\n\")\n",
    "for query in test_queries:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    \n",
    "    print(f\"Answer: {result['result']}\\n\")\n",
    "    print(f\"Sources: {len(result['source_documents'])} documents used\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. LangChain Agents with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tools for the agent\n",
    "def search_aws_docs(query: str) -> str:\n",
    "    \"\"\"Search AWS documentation\"\"\"\n",
    "    result = qa_chain.invoke({\"query\": query})\n",
    "    return result['result']\n",
    "\n",
    "def calculate_cost(service: str, usage: str) -> str:\n",
    "    \"\"\"Estimate AWS service costs\"\"\"\n",
    "    # Mock cost calculator\n",
    "    costs = {\n",
    "        \"S3\": \"$0.023 per GB/month for Standard storage\",\n",
    "        \"Lambda\": \"$0.20 per 1M requests + $0.0000166667 per GB-second\",\n",
    "        \"EC2\": \"Varies by instance type, e.g., t3.micro ~$0.0104/hour\",\n",
    "        \"DynamoDB\": \"On-demand: $1.25 per million write requests\"\n",
    "    }\n",
    "    return costs.get(service.upper(), \"Pricing information not available\")\n",
    "\n",
    "def get_service_info(service: str) -> str:\n",
    "    \"\"\"Get basic information about an AWS service\"\"\"\n",
    "    info = {\n",
    "        \"S3\": \"Object storage service\",\n",
    "        \"EC2\": \"Virtual servers in the cloud\",\n",
    "        \"Lambda\": \"Serverless compute service\",\n",
    "        \"RDS\": \"Managed relational database service\",\n",
    "        \"DynamoDB\": \"NoSQL database service\"\n",
    "    }\n",
    "    return info.get(service.upper(), \"Service information not available\")\n",
    "\n",
    "# Create tools\n",
    "tools = [\n",
    "    Tool(\n",
    "        name=\"AWS Documentation Search\",\n",
    "        func=search_aws_docs,\n",
    "        description=\"Search AWS documentation for detailed information about services, features, and best practices.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Cost Calculator\",\n",
    "        func=calculate_cost,\n",
    "        description=\"Get pricing information for AWS services. Input should be the service name.\"\n",
    "    ),\n",
    "    Tool(\n",
    "        name=\"Service Info\",\n",
    "        func=get_service_info,\n",
    "        description=\"Get quick information about what an AWS service does. Input should be the service name.\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(\"✓ Tools created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize agent with modern LangChain syntax\n",
    "# Create a simple ReAct prompt\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Define the ReAct prompt template\n",
    "react_prompt = PromptTemplate.from_template(\n",
    "    \"\"\"Answer the following questions as best you can. You have access to the following tools:\n",
    "\n",
    "{tools}\n",
    "\n",
    "Use the following format:\n",
    "\n",
    "Question: the input question you must answer\n",
    "Thought: you should always think about what to do\n",
    "Action: the action to take, should be one of [{tool_names}]\n",
    "Action Input: the input to the action\n",
    "Observation: the result of the action\n",
    "... (this Thought/Action/Action Input/Observation can repeat N times)\n",
    "Thought: I now know the final answer\n",
    "Final Answer: the final answer to the original input question\n",
    "\n",
    "Begin!\n",
    "\n",
    "Question: {input}\n",
    "Thought: {agent_scratchpad}\"\"\"\n",
    ")\n",
    "\n",
    "# Create the agent\n",
    "agent = create_react_agent(\n",
    "    llm=llm,\n",
    "    tools=tools,\n",
    "    prompt=react_prompt\n",
    ")\n",
    "\n",
    "# Create the agent executor\n",
    "agent_executor = AgentExecutor(\n",
    "    agent=agent,\n",
    "    tools=tools,\n",
    "    verbose=True,\n",
    "    max_iterations=5,\n",
    "    handle_parsing_errors=True\n",
    ")\n",
    "\n",
    "print(\"✓ LangChain Agent initialized (modern syntax)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the agent\n",
    "agent_queries = [\n",
    "    \"What is Lambda and how much does it cost?\",\n",
    "    \"I need storage for my application. What should I use and what's the pricing?\",\n",
    "    \"Tell me about DynamoDB's performance characteristics\"\n",
    "]\n",
    "\n",
    "print(\"LangChain Agent Responses:\\n\")\n",
    "for query in agent_queries:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"User Query: {query}\\n\")\n",
    "    \n",
    "    try:\n",
    "        response = agent_executor.invoke({\"input\": query})\n",
    "        print(f\"\\nFinal Answer: {response['output']}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LlamaIndex with Bedrock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. LlamaIndex Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install LlamaIndex packages\n",
    "!pip install -q llama-index llama-index-llms-bedrock llama-index-embeddings-bedrock\n",
    "\n",
    "print(\"✓ LlamaIndex packages installed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LlamaIndex imports\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Document, Settings\n",
    "from llama_index.core import StorageContext, load_index_from_storage\n",
    "from llama_index.core.node_parser import SimpleNodeParser\n",
    "from llama_index.llms.bedrock import Bedrock\n",
    "from llama_index.embeddings.bedrock import BedrockEmbedding\n",
    "from llama_index.core.retrievers import VectorIndexRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "from llama_index.core.response.pprint_utils import pprint_response\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"✓ LlamaIndex imports complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Configure LlamaIndex with Bedrock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Bedrock LLM for LlamaIndex\n",
    "llm_llamaindex = Bedrock(\n",
    "    model=\"anthropic.claude-3-haiku-20240307-v1:0\",\n",
    "    client=bedrock_runtime,\n",
    "    temperature=0.7,\n",
    "    max_tokens=512\n",
    ")\n",
    "\n",
    "# Initialize Bedrock Embeddings for LlamaIndex\n",
    "embed_model = BedrockEmbedding(\n",
    "    model=\"amazon.titan-embed-text-v1\",\n",
    "    client=bedrock_runtime\n",
    ")\n",
    "\n",
    "# Configure global settings\n",
    "Settings.llm = llm_llamaindex\n",
    "Settings.embed_model = embed_model\n",
    "Settings.chunk_size = 512\n",
    "Settings.chunk_overlap = 50\n",
    "\n",
    "print(\"✓ LlamaIndex configured with Bedrock\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test basic LLM invocation\n",
    "response = llm_llamaindex.complete(\"Explain Amazon Bedrock in one sentence.\")\n",
    "print(\"LlamaIndex LLM Response:\")\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Create Documents and Build Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create LlamaIndex documents from our AWS service descriptions\n",
    "llamaindex_documents = [\n",
    "    Document(text=\"\"\"Amazon S3 (Simple Storage Service) is an object storage service offering \n",
    "    industry-leading scalability, data availability, security, and performance. Customers of all \n",
    "    sizes and industries can use S3 to store and protect any amount of data for data lakes, \n",
    "    websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, \n",
    "    and big data analytics. S3 provides 99.999999999% (11 9's) durability.\"\"\",\n",
    "             metadata={\"service\": \"S3\", \"category\": \"storage\"}),\n",
    "    \n",
    "    Document(text=\"\"\"Amazon EC2 (Elastic Compute Cloud) provides secure, resizable compute capacity \n",
    "    in the cloud. EC2 presents a true virtual computing environment, allowing you to use web service \n",
    "    interfaces to launch instances with a variety of operating systems, load them with your custom \n",
    "    application environment, manage your network's access permissions, and run your image using as \n",
    "    many or few systems as you desire.\"\"\",\n",
    "             metadata={\"service\": \"EC2\", \"category\": \"compute\"}),\n",
    "    \n",
    "    Document(text=\"\"\"AWS Lambda is a serverless, event-driven compute service that lets you run code \n",
    "    for virtually any type of application or backend service without provisioning or managing servers. \n",
    "    Lambda runs your code on a high-availability compute infrastructure and performs all administration \n",
    "    of the compute resources, including server and operating system maintenance, capacity provisioning \n",
    "    and automatic scaling, and logging. You pay only for the compute time you consume.\"\"\",\n",
    "             metadata={\"service\": \"Lambda\", \"category\": \"compute\"}),\n",
    "    \n",
    "    Document(text=\"\"\"Amazon DynamoDB is a fully managed NoSQL database service that provides fast and \n",
    "    predictable performance with seamless scalability. DynamoDB lets you offload the administrative \n",
    "    burdens of operating and scaling a distributed database. DynamoDB automatically spreads the data \n",
    "    and traffic for your tables over a sufficient number of servers to handle your throughput and \n",
    "    storage requirements, while maintaining consistent and fast performance.\"\"\",\n",
    "             metadata={\"service\": \"DynamoDB\", \"category\": \"database\"}),\n",
    "    \n",
    "    Document(text=\"\"\"Amazon RDS (Relational Database Service) makes it easy to set up, operate, and \n",
    "    scale a relational database in the cloud. It provides cost-efficient and resizable capacity while \n",
    "    automating time-consuming administration tasks such as hardware provisioning, database setup, \n",
    "    patching and backups. RDS supports several database engines including MySQL, PostgreSQL, MariaDB, \n",
    "    Oracle, SQL Server, and Amazon Aurora.\"\"\",\n",
    "             metadata={\"service\": \"RDS\", \"category\": \"database\"}),\n",
    "    \n",
    "    Document(text=\"\"\"Amazon SageMaker is a fully managed machine learning service. With SageMaker, \n",
    "    data scientists and developers can quickly and easily build and train machine learning models, \n",
    "    and then directly deploy them into a production-ready hosted environment. SageMaker provides \n",
    "    built-in algorithms that are optimized to work efficiently against extremely large data in a \n",
    "    distributed environment.\"\"\",\n",
    "             metadata={\"service\": \"SageMaker\", \"category\": \"ml\"}),\n",
    "]\n",
    "\n",
    "print(f\"Created {len(llamaindex_documents)} LlamaIndex documents\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build vector index\n",
    "print(\"Building LlamaIndex vector store...\")\n",
    "index = VectorStoreIndex.from_documents(\n",
    "    llamaindex_documents,\n",
    "    show_progress=True\n",
    ")\n",
    "\n",
    "print(\"\\n✓ Vector index created successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Query the Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create query engine\n",
    "query_engine = index.as_query_engine(\n",
    "    similarity_top_k=3,\n",
    "    response_mode=\"compact\"\n",
    ")\n",
    "\n",
    "# Test queries\n",
    "test_queries_llama = [\n",
    "    \"What storage service should I use for a data lake?\",\n",
    "    \"How does serverless computing work on AWS?\",\n",
    "    \"Which database is best for high performance NoSQL?\",\n",
    "    \"What service helps with machine learning?\"\n",
    "]\n",
    "\n",
    "print(\"LlamaIndex Query Results:\\n\")\n",
    "for query in test_queries_llama:\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    \n",
    "    response = query_engine.query(query)\n",
    "    print(f\"Answer: {response}\\n\")\n",
    "    \n",
    "    # Show source nodes\n",
    "    print(\"Source Nodes:\")\n",
    "    for i, node in enumerate(response.source_nodes):\n",
    "        print(f\"  [{i+1}] Score: {node.score:.4f} - Service: {node.metadata.get('service', 'N/A')}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11. Advanced LlamaIndex Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom retriever with filtering\n",
    "retriever = VectorIndexRetriever(\n",
    "    index=index,\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "\n",
    "# Create custom query engine\n",
    "custom_query_engine = RetrieverQueryEngine(\n",
    "    retriever=retriever,\n",
    ")\n",
    "\n",
    "# Query with metadata filtering\n",
    "query = \"Tell me about compute services\"\n",
    "response = custom_query_engine.query(query)\n",
    "\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(f\"Response: {response}\\n\")\n",
    "print(\"Retrieved Nodes:\")\n",
    "for node in response.source_nodes:\n",
    "    print(f\"  - {node.metadata['service']} ({node.metadata['category']}) - Score: {node.score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Streaming responses\n",
    "streaming_engine = index.as_query_engine(\n",
    "    streaming=True,\n",
    "    similarity_top_k=2\n",
    ")\n",
    "\n",
    "query = \"Explain the benefits of using Amazon S3\"\n",
    "print(f\"Query: {query}\\n\")\n",
    "print(\"Streaming Response:\")\n",
    "\n",
    "streaming_response = streaming_engine.query(query)\n",
    "for text in streaming_response.response_gen:\n",
    "    print(text, end=\"\", flush=True)\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 12. Chat Engine with LlamaIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create chat engine\n",
    "chat_engine = index.as_chat_engine(\n",
    "    chat_mode=\"context\",\n",
    "    similarity_top_k=3,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Multi-turn conversation\n",
    "conversation = [\n",
    "    \"What is Amazon S3?\",\n",
    "    \"How durable is it?\",\n",
    "    \"What are some common use cases?\"\n",
    "]\n",
    "\n",
    "print(\"LlamaIndex Chat Engine:\\n\")\n",
    "for message in conversation:\n",
    "    print(f\"User: {message}\")\n",
    "    response = chat_engine.chat(message)\n",
    "    print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset chat and try a different topic\n",
    "chat_engine.reset()\n",
    "\n",
    "print(\"New Conversation:\\n\")\n",
    "new_conversation = [\n",
    "    \"I need to build a serverless application\",\n",
    "    \"What compute service should I use?\",\n",
    "    \"How does the pricing work for that?\"\n",
    "]\n",
    "\n",
    "for message in new_conversation:\n",
    "    print(f\"User: {message}\")\n",
    "    response = chat_engine.chat(message)\n",
    "    print(f\"Assistant: {response}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 13. Sub-Question Query Engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.query_engine import SubQuestionQueryEngine\n",
    "from llama_index.core.tools import QueryEngineTool, ToolMetadata\n",
    "\n",
    "# Create specialized query engines for different categories\n",
    "storage_docs = [d for d in llamaindex_documents if d.metadata['category'] == 'storage']\n",
    "compute_docs = [d for d in llamaindex_documents if d.metadata['category'] == 'compute']\n",
    "database_docs = [d for d in llamaindex_documents if d.metadata['category'] == 'database']\n",
    "\n",
    "storage_index = VectorStoreIndex.from_documents(storage_docs)\n",
    "compute_index = VectorStoreIndex.from_documents(compute_docs)\n",
    "database_index = VectorStoreIndex.from_documents(database_docs)\n",
    "\n",
    "# Create query engine tools\n",
    "query_engine_tools = [\n",
    "    QueryEngineTool(\n",
    "        query_engine=storage_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"storage_expert\",\n",
    "            description=\"Expert on AWS storage services like S3\"\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=compute_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"compute_expert\",\n",
    "            description=\"Expert on AWS compute services like EC2 and Lambda\"\n",
    "        )\n",
    "    ),\n",
    "    QueryEngineTool(\n",
    "        query_engine=database_index.as_query_engine(),\n",
    "        metadata=ToolMetadata(\n",
    "            name=\"database_expert\",\n",
    "            description=\"Expert on AWS database services like RDS and DynamoDB\"\n",
    "        )\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create sub-question query engine\n",
    "sub_question_engine = SubQuestionQueryEngine.from_defaults(\n",
    "    query_engine_tools=query_engine_tools,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"✓ Sub-question query engine created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test complex query that requires multiple sub-questions\n",
    "complex_query = \"\"\"I want to build a web application that needs storage for user uploads, \n",
    "serverless compute for processing, and a database for user data. What AWS services should I use?\"\"\"\n",
    "\n",
    "print(f\"Complex Query: {complex_query}\\n\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "response = sub_question_engine.query(complex_query)\n",
    "print(f\"\\nFinal Answer:\\n{response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Framework Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 14. LangChain vs LlamaIndex Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# Comparison test queries\n",
    "comparison_queries = [\n",
    "    \"What is Amazon S3 used for?\",\n",
    "    \"How does Lambda pricing work?\",\n",
    "    \"Which database service is best for scalability?\"\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "print(\"Running Framework Comparison...\\n\")\n",
    "\n",
    "for query in comparison_queries:\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # LangChain\n",
    "    start = time.time()\n",
    "    lc_result = qa_chain.invoke({\"query\": query})\n",
    "    lc_time = time.time() - start\n",
    "    lc_response = lc_result['result']\n",
    "    \n",
    "    # LlamaIndex\n",
    "    start = time.time()\n",
    "    li_result = query_engine.query(query)\n",
    "    li_time = time.time() - start\n",
    "    li_response = str(li_result)\n",
    "    \n",
    "    results.append({\n",
    "        'query': query,\n",
    "        'langchain_time': lc_time,\n",
    "        'llamaindex_time': li_time,\n",
    "        'langchain_response_length': len(lc_response),\n",
    "        'llamaindex_response_length': len(li_response),\n",
    "    })\n",
    "    \n",
    "    print(f\"  LangChain: {lc_time:.2f}s\")\n",
    "    print(f\"  LlamaIndex: {li_time:.2f}s\\n\")\n",
    "\n",
    "# Create comparison DataFrame\n",
    "comparison_df = pd.DataFrame(results)\n",
    "print(\"\\nPerformance Comparison:\")\n",
    "print(comparison_df.to_string(index=False))\n",
    "\n",
    "print(f\"\\nAverage Times:\")\n",
    "print(f\"  LangChain: {comparison_df['langchain_time'].mean():.3f}s\")\n",
    "print(f\"  LlamaIndex: {comparison_df['llamaindex_time'].mean():.3f}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize comparison\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Response time comparison\n",
    "x = np.arange(len(comparison_queries))\n",
    "width = 0.35\n",
    "\n",
    "axes[0].bar(x - width/2, comparison_df['langchain_time'], width, label='LangChain', alpha=0.8)\n",
    "axes[0].bar(x + width/2, comparison_df['llamaindex_time'], width, label='LlamaIndex', alpha=0.8)\n",
    "axes[0].set_xlabel('Query')\n",
    "axes[0].set_ylabel('Time (seconds)')\n",
    "axes[0].set_title('Response Time Comparison', fontweight='bold')\n",
    "axes[0].set_xticks(x)\n",
    "axes[0].set_xticklabels([f'Q{i+1}' for i in range(len(comparison_queries))])\n",
    "axes[0].legend()\n",
    "axes[0].grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Response length comparison\n",
    "axes[1].bar(x - width/2, comparison_df['langchain_response_length'], width, label='LangChain', alpha=0.8)\n",
    "axes[1].bar(x + width/2, comparison_df['llamaindex_response_length'], width, label='LlamaIndex', alpha=0.8)\n",
    "axes[1].set_xlabel('Query')\n",
    "axes[1].set_ylabel('Response Length (characters)')\n",
    "axes[1].set_title('Response Length Comparison', fontweight='bold')\n",
    "axes[1].set_xticks(x)\n",
    "axes[1].set_xticklabels([f'Q{i+1}' for i in range(len(comparison_queries))])\n",
    "axes[1].legend()\n",
    "axes[1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 15. Framework Feature Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature comparison table\n",
    "features = {\n",
    "    'Feature': [\n",
    "        'Bedrock Integration',\n",
    "        'Vector Store Support',\n",
    "        'Memory/Chat History',\n",
    "        'Agent Support',\n",
    "        'Streaming',\n",
    "        'Custom Prompts',\n",
    "        'Multi-Modal',\n",
    "        'Evaluation Tools',\n",
    "        'Learning Curve',\n",
    "        'Best For'\n",
    "    ],\n",
    "    'LangChain': [\n",
    "        '✓ Native Support',\n",
    "        '✓ Multiple (FAISS, Pinecone, etc.)',\n",
    "        '✓ Multiple Memory Types',\n",
    "        '✓ Rich Agent Framework',\n",
    "        '✓ Supported',\n",
    "        '✓ Flexible Templates',\n",
    "        '✓ Supported',\n",
    "        '✓ Built-in',\n",
    "        'Moderate',\n",
    "        'Complex workflows, agents, chains'\n",
    "    ],\n",
    "    'LlamaIndex': [\n",
    "        '✓ Native Support',\n",
    "        '✓ Multiple Vector Stores',\n",
    "        '✓ Chat Engine',\n",
    "        '✓ Query Engines',\n",
    "        '✓ Supported',\n",
    "        '✓ Customizable',\n",
    "        '✓ Supported',\n",
    "        '✓ Built-in',\n",
    "        'Easy',\n",
    "        'RAG, document Q&A, search'\n",
    "    ]\n",
    "}\n",
    "\n",
    "features_df = pd.DataFrame(features)\n",
    "print(\"\\nLangChain vs LlamaIndex - Feature Comparison:\\n\")\n",
    "print(features_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 16. Use Case Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommendations = \"\"\"\n",
    "╔══════════════════════════════════════════════════════════════════════════════╗\n",
    "║                    FRAMEWORK SELECTION GUIDE                                  ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  Choose LANGCHAIN when:                                                       ║\n",
    "║  ✓ Building complex multi-step workflows                                     ║\n",
    "║  ✓ Need sophisticated agent behavior                                         ║\n",
    "║  ✓ Require extensive chain composition                                       ║\n",
    "║  ✓ Working with multiple tools and APIs                                      ║\n",
    "║  ✓ Need flexible memory management                                           ║\n",
    "║  ✓ Building conversational AI applications                                   ║\n",
    "║                                                                               ║\n",
    "║  Example Use Cases:                                                           ║\n",
    "║  • Customer service chatbots with tool use                                   ║\n",
    "║  • Multi-agent systems                                                       ║\n",
    "║  • Complex decision-making workflows                                         ║\n",
    "║  • Integration with external APIs and databases                              ║\n",
    "║                                                                               ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  Choose LLAMAINDEX when:                                                      ║\n",
    "║  ✓ Primary focus is document search and retrieval                            ║\n",
    "║  ✓ Building knowledge bases or Q&A systems                                   ║\n",
    "║  ✓ Need advanced indexing strategies                                         ║\n",
    "║  ✓ Working primarily with structured/unstructured documents                  ║\n",
    "║  ✓ Want simpler, more focused API                                            ║\n",
    "║  ✓ Need fast prototyping for RAG applications                                ║\n",
    "║                                                                               ║\n",
    "║  Example Use Cases:                                                           ║\n",
    "║  • Internal documentation search                                             ║\n",
    "║  • Research paper Q&A systems                                                ║\n",
    "║  • Product documentation assistants                                          ║\n",
    "║  • Knowledge management systems                                              ║\n",
    "║                                                                               ║\n",
    "╠══════════════════════════════════════════════════════════════════════════════╣\n",
    "║                                                                               ║\n",
    "║  Use BOTH when:                                                               ║\n",
    "║  ✓ Need best of both worlds (they're compatible!)                            ║\n",
    "║  ✓ Complex RAG with sophisticated agent behavior                             ║\n",
    "║  ✓ Large-scale production systems                                            ║\n",
    "║                                                                               ║\n",
    "╚══════════════════════════════════════════════════════════════════════════════╝\n",
    "\"\"\"\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 17. Cost Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estimate lab costs\n",
    "cost_breakdown = {\n",
    "    'Component': [\n",
    "        'LangChain - Claude Haiku Calls',\n",
    "        'LangChain - Titan Embeddings',\n",
    "        'LangChain - Agent Calls',\n",
    "        'LlamaIndex - Claude Haiku Calls',\n",
    "        'LlamaIndex - Titan Embeddings',\n",
    "        'LlamaIndex - Chat Engine',\n",
    "        'Comparison Tests'\n",
    "    ],\n",
    "    'Estimated Calls': [15, 20, 10, 15, 25, 8, 6],\n",
    "    'Avg Cost per Call': [0.0003, 0.000005, 0.0004, 0.0003, 0.000005, 0.0003, 0.0003],\n",
    "}\n",
    "\n",
    "cost_df = pd.DataFrame(cost_breakdown)\n",
    "cost_df['Total Cost'] = cost_df['Estimated Calls'] * cost_df['Avg Cost per Call']\n",
    "\n",
    "print(\"Lab 4 - Cost Breakdown:\\n\")\n",
    "print(cost_df.to_string(index=False))\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"Total Estimated Cost: ${cost_df['Total Cost'].sum():.4f}\")\n",
    "print(f\"\\n✓ Well under $1.00 budget!\")\n",
    "\n",
    "print(\"\\nCost Optimization Tips:\")\n",
    "print(\"  1. Use Claude Haiku for most tasks (10x cheaper than Sonnet)\")\n",
    "print(\"  2. Cache embeddings when possible\")\n",
    "print(\"  3. Reuse vector stores instead of rebuilding\")\n",
    "print(\"  4. Use appropriate chunk sizes to minimize tokens\")\n",
    "print(\"  5. Implement query caching for common questions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this lab, you learned:\n",
    "\n",
    "**LangChain with Bedrock:**\n",
    "- ✅ Integrating Bedrock models with LangChain\n",
    "- ✅ Building chains with prompt templates\n",
    "- ✅ Implementing conversation memory\n",
    "- ✅ Creating RAG systems with FAISS\n",
    "- ✅ Building agents with tool use\n",
    "\n",
    "**LlamaIndex with Bedrock:**\n",
    "- ✅ Configuring LlamaIndex with Bedrock LLMs and embeddings\n",
    "- ✅ Building vector indexes from documents\n",
    "- ✅ Implementing query engines\n",
    "- ✅ Creating chat engines with memory\n",
    "- ✅ Using sub-question query decomposition\n",
    "\n",
    "**Framework Comparison:**\n",
    "- ✅ Performance benchmarking\n",
    "- ✅ Feature comparison\n",
    "- ✅ Use case recommendations\n",
    "- ✅ Best practices for each framework\n",
    "\n",
    "**Key Takeaways:**\n",
    "1. LangChain excels at complex workflows and agents\n",
    "2. LlamaIndex is optimized for RAG and document search\n",
    "3. Both frameworks integrate seamlessly with Bedrock\n",
    "4. Choose based on your specific use case\n",
    "5. You can use both frameworks together!\n",
    "\n",
    "**Next Steps:**\n",
    "- Build production applications with your chosen framework\n",
    "- Explore advanced features (streaming, async, etc.)\n",
    "- Implement monitoring and evaluation\n",
    "- Scale to production workloads\n",
    "\n",
    "**Additional Resources:**\n",
    "- [LangChain Documentation](https://python.langchain.com/docs/get_started/introduction)\n",
    "- [LlamaIndex Documentation](https://docs.llamaindex.ai/)\n",
    "- [Bedrock LangChain Integration](https://python.langchain.com/docs/integrations/llms/bedrock)\n",
    "- [Bedrock LlamaIndex Integration](https://docs.llamaindex.ai/en/stable/examples/llm/bedrock/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}